{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkxRSYjzA1oX"
   },
   "source": [
    "##### Copyright 2025 Google LLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2025-03-31T10:03:40.865454Z",
     "iopub.status.busy": "2025-03-31T10:03:40.865029Z",
     "iopub.status.idle": "2025-03-31T10:03:40.871949Z",
     "shell.execute_reply": "2025-03-31T10:03:40.870628Z",
     "shell.execute_reply.started": "2025-03-31T10:03:40.865391Z"
    },
    "id": "5u5OZ2ShA3BA",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# @title Licenciado bajo la Licencia Apache, Versión 2.0 (la \"Licencia\");\n",
    "# no puedes usar este archivo excepto en cumplimiento con la Licencia.\n",
    "# Puedes obtener una copia de la Licencia en\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# A menos que sea requerido por la ley aplicable o acordado por escrito, el software\n",
    "# distribuido bajo la Licencia se distribuye \"TAL CUAL\",\n",
    "# SIN GARANTÍAS O CONDICIONES DE NINGÚN TIPO, ya sean expresas o implícitas.\n",
    "# Consulta la Licencia para conocer el lenguaje específico que rige los permisos y\n",
    "# limitaciones bajo la Licencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csNPnkuCobmG"
   },
   "source": [
    "# Día 1 - Creación de Prompts\n",
    "\n",
    "¡Bienvenido al curso de 5 días de Generative AI en Kaggle!\n",
    "\n",
    "Este notebook te mostrará cómo comenzar con la API de Gemini y te guiará a través de algunos ejemplos de prompts y técnicas que también puedes leer en el documento técnico sobre Prompting. No necesitas leer el documento técnico para usar este notebook, pero los documentos te darán un contexto teórico y antecedentes para complementar este notebook interactivo.\n",
    "\n",
    "\n",
    "## Antes de comenzar\n",
    "\n",
    "En este notebook, comenzarás a explorar la creación de prompts utilizando el SDK de Python y AI Studio. Para inspirarte, podrías disfrutar explorando algunas aplicaciones que se han creado utilizando la familia de modelos Gemini. Aquí hay algunas que nos gustan, y creemos que a ti también te gustarán.\n",
    "\n",
    "* [TextFX](https://textfx.withgoogle.com/) es un conjunto de herramientas impulsadas por IA para raperos, creado en colaboración con Lupe Fiasco,\n",
    "* [SQL Talk](https://sql-talk-r5gdynozbq-uc.a.run.app/) muestra cómo puedes hablar directamente con una base de datos utilizando la API de Gemini,\n",
    "* [NotebookLM](https://notebooklm.google/) utiliza modelos Gemini para construir tu propio asistente de investigación personal impulsado por IA.\n",
    "\n",
    "\n",
    "## Para obtener ayuda\n",
    "\n",
    "**Los problemas comunes están cubiertos en la [guía de preguntas frecuentes y solución de problemas](https://www.kaggle.com/code/markishere/day-0-troubleshooting-and-faqs).**\n",
    "\n",
    "## ¡Novedades de Gemini 2.0!\n",
    "\n",
    "Este material del curso se lanzó por primera vez en noviembre de 2024. El espacio de IA y LLM está avanzando increíblemente rápido, por lo que hemos realizado algunas actualizaciones para usar los últimos modelos y capacidades.\n",
    "\n",
    "* Estos laboratorios de código se han actualizado para usar la familia de modelos Gemini 2.0.\n",
    "* El SDK de Python se ha actualizado de `google-generativeai` al nuevo y unificado SDK [`google-genai`](https://pypi.org/project/google-genai).\n",
    "  * Este nuevo SDK funciona tanto con la API de desarrolladores de Gemini como con Google Cloud Vertex AI, y cambiar entre ellos es [tan simple como cambiar algunos campos](https://pypi.org/project/google-genai/#:~:text=.Client%28%29-,API%20Selection,-By%20default%2C%20the).\n",
    "* Se han agregado nuevas capacidades de modelo a los laboratorios de código relevantes, como el \"modo de pensamiento\" en este laboratorio.\n",
    "* El Día 1 incluye un nuevo [laboratorio de evaluación](https://www.kaggle.com/code/markishere/day-1-evaluation-and-structured-output)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f00ea7c99e44"
   },
   "source": [
    "## Comenzar con los notebooks de Kaggle\n",
    "\n",
    "Si es la primera vez que usas un notebook de Kaggle, ¡bienvenido! Puedes leer sobre cómo usar los notebooks de Kaggle [en la documentación](https://www.kaggle.com/docs/notebooks).\n",
    "\n",
    "Primero, necesitarás verificar tu cuenta por teléfono en kaggle.com/settings.\n",
    "\n",
    "![](https://storage.googleapis.com/kaggle-media/Images/5dgai_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ejecutar este notebook, así como los demás en este curso, necesitarás hacer una copia o bifurcar el notebook. Busca el botón `Copy and Edit` en la parte superior derecha y **haz clic en él** para hacer una copia privada editable del notebook. Debería verse como este:\n",
    "\n",
    "![Botón Copy and Edit](https://storage.googleapis.com/kaggle-media/Images/5gdai_sc_1.png)\n",
    "\n",
    "Tu copia ahora tendrá un botón ▶️ **Run** junto a cada celda de código que puedes presionar para ejecutar esa celda. Se espera que estos notebooks se ejecuten en orden de arriba hacia abajo, pero se te anima a agregar nuevas celdas, ejecutar tu propio código y explorar. Si te quedas atascado, puedes intentar la opción `Factory reset` en el menú `Run`, o volver al notebook original y hacer una nueva copia.\n",
    "\n",
    "![Botón Run cell](https://storage.googleapis.com/kaggle-media/Images/5gdai_sc_2.png)\n",
    "\n",
    "### ¿Problemas?\n",
    "\n",
    "Si tienes algún problema, dirígete al [Discord de Kaggle](https://discord.com/invite/kaggle), encuentra el canal [`#5dgai-q-and-a`](https://discord.com/channels/1101210829807956100/1303438695143178251) y pide ayuda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExkOXcPxtTb5"
   },
   "source": [
    "## Comenzar con la API de Gemini\n",
    "\n",
    "Todos los ejercicios en este notebook utilizarán la [API de Gemini](https://ai.google.dev/gemini-api/) a través del [SDK de Python](https://pypi.org/project/google-genai/). Cada uno de estos prompts se puede acceder directamente en [Google AI Studio](https://aistudio.google.com/) también, por lo que si prefieres usar una interfaz web y omitir el código para esta actividad, busca el enlace <img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> AI Studio en cada prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, necesitarás agregar tu clave API a tu Notebook de Kaggle como un Secreto de Usuario de Kaggle.\n",
    "\n",
    "![](https://storage.googleapis.com/kaggle-media/Images/5dgai_1.png)\n",
    "![](https://storage.googleapis.com/kaggle-media/Images/5dgai_2.png)\n",
    "![](https://storage.googleapis.com/kaggle-media/Images/5dgai_3.png)\n",
    "![](https://storage.googleapis.com/kaggle-media/Images/5dgai_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAjUV3BsvFXQ"
   },
   "source": [
    "### Instalar el SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:03:40.874310Z",
     "iopub.status.busy": "2025-03-31T10:03:40.873953Z",
     "iopub.status.idle": "2025-03-31T10:03:53.209301Z",
     "shell.execute_reply": "2025-03-31T10:03:53.207859Z",
     "shell.execute_reply.started": "2025-03-31T10:03:40.874270Z"
    },
    "id": "NzwzJFU9LqkJ"
   },
   "outputs": [],
   "source": [
    "!pip uninstall -qqy jupyterlab  # Remove unused packages from Kaggle's base image that conflict\n",
    "!pip install -U -q \"google-genai==1.7.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importa el SDK y algunos ayudantes para renderizar la salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:03:53.211769Z",
     "iopub.status.busy": "2025-03-31T10:03:53.211256Z",
     "iopub.status.idle": "2025-03-31T10:03:54.638132Z",
     "shell.execute_reply": "2025-03-31T10:03:54.636956Z",
     "shell.execute_reply.started": "2025-03-31T10:03:53.211723Z"
    },
    "id": "5DwxYIRavMST"
   },
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "from IPython.display import HTML, Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configura un ayudante de reintento. Esto te permite \"Ejecutar todo\" sin preocuparte por la cuota por minuto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:03:54.640904Z",
     "iopub.status.busy": "2025-03-31T10:03:54.640394Z",
     "iopub.status.idle": "2025-03-31T10:03:54.845515Z",
     "shell.execute_reply": "2025-03-31T10:03:54.844479Z",
     "shell.execute_reply.started": "2025-03-31T10:03:54.640869Z"
    }
   },
   "outputs": [],
   "source": [
    "from google.api_core import retry\n",
    "\n",
    "\n",
    "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
    "\n",
    "genai.models.Models.generate_content = retry.Retry(\n",
    "    predicate=is_retriable)(genai.models.Models.generate_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNEt2BCOvOJ1"
   },
   "source": [
    "### Configura tu clave API\n",
    "\n",
    "Para ejecutar la siguiente celda, tu clave API debe estar almacenada en un [secreto de Kaggle](https://www.kaggle.com/discussions/product-feedback/114053) llamado `GOOGLE_API_KEY`.\n",
    "\n",
    "Si aún no tienes una clave API, puedes obtener una en [AI Studio](https://aistudio.google.com/app/apikey). Puedes encontrar [instrucciones detalladas en la documentación](https://ai.google.dev/gemini-api/docs/api-key).\n",
    "\n",
    "Para hacer que la clave esté disponible a través de secretos de Kaggle, elige `Secrets` en el menú `Add-ons` y sigue las instrucciones para agregar tu clave o habilitarla para este notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:03:54.847270Z",
     "iopub.status.busy": "2025-03-31T10:03:54.846853Z",
     "iopub.status.idle": "2025-03-31T10:03:55.056841Z",
     "shell.execute_reply": "2025-03-31T10:03:55.055686Z",
     "shell.execute_reply.started": "2025-03-31T10:03:54.847240Z"
    },
    "id": "SHl0bkPCvayd"
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4e720472fd86"
   },
   "source": [
    "Si recibiste una respuesta de error del tipo `No user secrets exist for kernel id ...`, entonces necesitas agregar tu clave API a través de `Add-ons`, `Secrets` **y** habilitarla.\n",
    "\n",
    "![Captura de pantalla de la casilla de verificación para habilitar el secreto GOOGLE_API_KEY](https://storage.googleapis.com/kaggle-media/Images/5gdai_sc_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_YXCYIKvyZJ"
   },
   "source": [
    "### Ejecuta tu primer prompt\n",
    "\n",
    "En este paso, probarás que tu clave API esté configurada correctamente haciendo una solicitud.\n",
    "\n",
    "El SDK de Python utiliza un [`Client` object](https://googleapis.github.io/python-genai/genai.html#genai.client.Client) para hacer solicitudes a la API. El cliente te permite controlar qué back-end usar (entre la API de Gemini y Vertex AI) y maneja la autenticación (la clave API).\n",
    "\n",
    "El modelo `gemini-2.0-flash` ha sido seleccionado aquí.\n",
    "\n",
    "**Nota**: Si ves un `TransportError` en este paso, es posible que necesites **🔁 Factory reset** el notebook una vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:03:55.058716Z",
     "iopub.status.busy": "2025-03-31T10:03:55.058250Z",
     "iopub.status.idle": "2025-03-31T10:03:57.886958Z",
     "shell.execute_reply": "2025-03-31T10:03:57.885798Z",
     "shell.execute_reply.started": "2025-03-31T10:03:55.058668Z"
    },
    "id": "BV1o0PmcvyJF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, imagine you have a really, REALLY smart robot. But instead of being made of metal and wires, this smartness lives inside a computer!\n",
      "\n",
      "This smartness is called Artificial Intelligence, or AI for short.\n",
      "\n",
      "Think of it like this:\n",
      "\n",
      "*   **A puppy learning tricks:** You teach a puppy by showing it what to do and rewarding it when it does it right. AI learns in a similar way, but with lots and lots of examples and by getting \"rewards\" for making the right choices.\n",
      "*   **A super-powered calculator:** A calculator can do math really fast. AI is like a super-powered calculator that can do lots more than just math! It can see pictures, understand words, and even play games.\n",
      "*   **Playing hide-and-seek:** If you play hide-and-seek a lot, you get better at finding good hiding spots. AI is the same. The more it \"plays\" (or works), the better it gets at things!\n",
      "\n",
      "**So, what can AI do?**\n",
      "\n",
      "*   **Help you find videos you like:** Have you ever noticed that YouTube suggests videos you might enjoy? That's AI! It's learned what kind of videos you watch and tries to guess what you'll like next.\n",
      "*   **Help you spell words:** When you're typing and you misspell a word, sometimes your computer fixes it. That's AI helping you!\n",
      "*   **Help doctors find diseases:** AI can look at X-rays and other medical images to help doctors find problems that might be hard to see with their own eyes.\n",
      "*   **Drive cars:** Some cars are starting to drive themselves using AI! They can see the road, other cars, and even pedestrians.\n",
      "\n",
      "**AI is still learning and growing, just like you!** It's a tool that can help us do amazing things, but it's important to remember that it's not perfect. We still need people to help it learn and make sure it's used for good.\n",
      "\n",
      "Does that make sense? Do you have any other questions about AI?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=\"Explícame la IA como si fuera un niño.\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f60ed9d8ae41"
   },
   "source": [
    "La respuesta a menudo viene en formato markdown, que puedes renderizar directamente en este notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:03:57.888876Z",
     "iopub.status.busy": "2025-03-31T10:03:57.888544Z",
     "iopub.status.idle": "2025-03-31T10:03:57.897544Z",
     "shell.execute_reply": "2025-03-31T10:03:57.896275Z",
     "shell.execute_reply.started": "2025-03-31T10:03:57.888845Z"
    },
    "id": "c933e5e460a5"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, imagine you have a really, REALLY smart robot. But instead of being made of metal and wires, this smartness lives inside a computer!\n",
       "\n",
       "This smartness is called Artificial Intelligence, or AI for short.\n",
       "\n",
       "Think of it like this:\n",
       "\n",
       "*   **A puppy learning tricks:** You teach a puppy by showing it what to do and rewarding it when it does it right. AI learns in a similar way, but with lots and lots of examples and by getting \"rewards\" for making the right choices.\n",
       "*   **A super-powered calculator:** A calculator can do math really fast. AI is like a super-powered calculator that can do lots more than just math! It can see pictures, understand words, and even play games.\n",
       "*   **Playing hide-and-seek:** If you play hide-and-seek a lot, you get better at finding good hiding spots. AI is the same. The more it \"plays\" (or works), the better it gets at things!\n",
       "\n",
       "**So, what can AI do?**\n",
       "\n",
       "*   **Help you find videos you like:** Have you ever noticed that YouTube suggests videos you might enjoy? That's AI! It's learned what kind of videos you watch and tries to guess what you'll like next.\n",
       "*   **Help you spell words:** When you're typing and you misspell a word, sometimes your computer fixes it. That's AI helping you!\n",
       "*   **Help doctors find diseases:** AI can look at X-rays and other medical images to help doctors find problems that might be hard to see with their own eyes.\n",
       "*   **Drive cars:** Some cars are starting to drive themselves using AI! They can see the road, other cars, and even pedestrians.\n",
       "\n",
       "**AI is still learning and growing, just like you!** It's a tool that can help us do amazing things, but it's important to remember that it's not perfect. We still need people to help it learn and make sure it's used for good.\n",
       "\n",
       "Does that make sense? Do you have any other questions about AI?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byx0pT9ZMW2Q"
   },
   "source": [
    "### Inicia un chat\n",
    "\n",
    "El ejemplo anterior utiliza una estructura de un solo turno, entrada de texto/salida de texto, pero también puedes configurar una estructura de chat de varios turnos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:03:57.899193Z",
     "iopub.status.busy": "2025-03-31T10:03:57.898888Z",
     "iopub.status.idle": "2025-03-31T10:03:58.463778Z",
     "shell.execute_reply": "2025-03-31T10:03:58.462774Z",
     "shell.execute_reply.started": "2025-03-31T10:03:57.899163Z"
    },
    "id": "lV_S5ZL5MidD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice to meet you, Zlork! It's great to have you here. How can I help you today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat = client.chats.create(model='gemini-2.0-flash', history=[])\n",
    "response = chat.send_message('¡Hola! Mi nombre es Zlork.')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:03:58.465802Z",
     "iopub.status.busy": "2025-03-31T10:03:58.465343Z",
     "iopub.status.idle": "2025-03-31T10:04:00.174979Z",
     "shell.execute_reply": "2025-03-31T10:04:00.173771Z",
     "shell.execute_reply.started": "2025-03-31T10:03:58.465753Z"
    },
    "id": "7b0372c3c64a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's something interesting about dinosaurs:\n",
      "\n",
      "**There's evidence that some dinosaurs were brightly colored, possibly with iridescent feathers, and even different colors within the same species, similar to modern birds!**\n",
      "\n",
      "For a long time, scientists believed dinosaurs were largely drab in color, like reptiles. However, through analyzing fossilized feathers and scales, and even the microscopic structures called melanosomes (which contain pigment), researchers have been able to infer the colors of some dinosaurs.\n",
      "\n",
      "This has led to the discovery that some dinosaurs might have had brilliant hues, stripes, spots, and even iridescent sheens like a hummingbird. The *Microraptor*, for example, is thought to have had iridescent black feathers. And the *Sinosauropteryx* is believed to have had reddish-brown stripes.\n",
      "\n",
      "This discovery changes our understanding of dinosaur behavior, suggesting that color may have played a role in camouflage, communication, and courtship, just like it does in modern birds.\n",
      "\n",
      "So, the next time you picture a dinosaur, don't just think of a giant, green lizard. Imagine it possibly strutting around with vibrant, eye-catching colors!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message('¿Puedes decirme algo interesante sobre los dinosaurios?')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mientras tengas el objeto `chat` activo, el estado de la conversación\n",
    "persiste. Confirma eso preguntando si recuerda el nombre del usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:04:00.178763Z",
     "iopub.status.busy": "2025-03-31T10:04:00.178357Z",
     "iopub.status.idle": "2025-03-31T10:04:00.597558Z",
     "shell.execute_reply": "2025-03-31T10:04:00.596167Z",
     "shell.execute_reply.started": "2025-03-31T10:04:00.178728Z"
    },
    "id": "d3f9591392a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, your name is Zlork.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message('¿Recuerdas cuál es mi nombre?')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KAjpr1200sW"
   },
   "source": [
    "### Elige un modelo\n",
    "\n",
    "La API de Gemini proporciona acceso a varios modelos de la familia de modelos Gemini. Lee sobre los modelos disponibles y sus capacidades en la [página de descripción general del modelo](https://ai.google.dev/gemini-api/docs/models/gemini).\n",
    "\n",
    "En este paso, usarás la API para listar todos los modelos disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:04:00.599723Z",
     "iopub.status.busy": "2025-03-31T10:04:00.599246Z",
     "iopub.status.idle": "2025-03-31T10:04:00.631963Z",
     "shell.execute_reply": "2025-03-31T10:04:00.630701Z",
     "shell.execute_reply.started": "2025-03-31T10:04:00.599654Z"
    },
    "id": "uUUZa2uq2jDm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001\n",
      "models/text-bison-001\n",
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/gemini-2.5-pro-exp-03-25\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/learnlm-1.5-pro-experimental\n",
      "models/gemma-3-27b-it\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/aqa\n",
      "models/imagen-3.0-generate-002\n"
     ]
    }
   ],
   "source": [
    "for model in client.models.list():\n",
    "  print(model.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rN49kSI54R1v"
   },
   "source": [
    "La respuesta de [`models.list`](https://ai.google.dev/api/models#method:-models.list) también devuelve información adicional sobre las capacidades del modelo, como los límites de tokens y los parámetros compatibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:04:00.633866Z",
     "iopub.status.busy": "2025-03-31T10:04:00.633411Z",
     "iopub.status.idle": "2025-03-31T10:04:00.665926Z",
     "shell.execute_reply": "2025-03-31T10:04:00.664681Z",
     "shell.execute_reply.started": "2025-03-31T10:04:00.633819Z"
    },
    "id": "k7JJ1K6j4Rl8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': 'Gemini 2.0 Flash',\n",
      " 'display_name': 'Gemini 2.0 Flash',\n",
      " 'input_token_limit': 1048576,\n",
      " 'name': 'models/gemini-2.0-flash',\n",
      " 'output_token_limit': 8192,\n",
      " 'supported_actions': ['generateContent', 'countTokens'],\n",
      " 'tuned_model_info': {},\n",
      " 'version': '2.0'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for model in client.models.list():\n",
    "  if model.name == 'models/gemini-2.0-flash':\n",
    "    pprint(model.to_json_dict())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rU_UBlZdooM"
   },
   "source": [
    "## Explora los parámetros de generación\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7NfEizeipbW"
   },
   "source": [
    "### Longitud de salida\n",
    "\n",
    "Al generar texto con un LLM, la longitud de la salida afecta el costo y el rendimiento. Generar más tokens aumenta la computación, lo que lleva a un mayor consumo de energía, latencia y costo.\n",
    "\n",
    "Para detener el modelo de generar tokens más allá de un límite, puedes especificar el parámetro `max_output_tokens` al usar la API de Gemini. Especificar este parámetro no influye en la generación de los tokens de salida, por lo que la salida no se volverá más concisa estilística o textualmente, pero dejará de generar tokens una vez que se alcance la longitud especificada. Puede ser necesario el diseño de prompts para generar una salida más completa para tu límite dado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:04:00.667882Z",
     "iopub.status.busy": "2025-03-31T10:04:00.667458Z",
     "iopub.status.idle": "2025-03-31T10:04:02.147176Z",
     "shell.execute_reply": "2025-03-31T10:04:02.145805Z",
     "shell.execute_reply.started": "2025-03-31T10:04:00.667834Z"
    },
    "id": "qVf23JsIi9ma"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## The Humble Olive: A Cornerstone of Modern Society\n",
      "\n",
      "The olive, a small, often brine-soaked fruit, may seem insignificant amidst the complexities of modern society. Yet, beneath its unassuming exterior lies a profound importance, extending far beyond its culinary applications. From its historical roots to its contributions to health, economy, and culture, the olive tree and its fruit have woven themselves into the fabric of modern life, demonstrating an enduring relevance that belies its perceived simplicity.\n",
      "\n",
      "The story of the olive is deeply intertwined with the history of civilization. Originating in the Mediterranean basin thousands of years ago, the olive tree, *Olea europaea*, quickly became a cornerstone of ancient economies and societies. Olive oil was not merely a foodstuff; it was a source of fuel for lamps, a component of religious rituals, a base for perfumes and medicines, and a symbol of peace and prosperity. Ancient Greece valued the olive tree so highly that it was considered sacred to the goddess Athena, and victors in\n"
     ]
    }
   ],
   "source": [
    "from google.genai import types\n",
    "\n",
    "short_config = types.GenerateContentConfig(max_output_tokens=200)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=short_config,\n",
    "    contents='Escribe un ensayo de 1000 palabras sobre la importancia de las aceitunas en la sociedad moderna.')\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:04:02.149272Z",
     "iopub.status.busy": "2025-03-31T10:04:02.148834Z",
     "iopub.status.idle": "2025-03-31T10:04:02.932445Z",
     "shell.execute_reply": "2025-03-31T10:04:02.931442Z",
     "shell.execute_reply.started": "2025-03-31T10:04:02.149225Z"
    },
    "id": "W-3kR2F5kdMR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From groves sun-drenched and ancient roots,\n",
      "The olive's fruit, a bounty shoots.\n",
      "Pressed into oil, a golden stream,\n",
      "Nourishing bodies, fulfilling dream.\n",
      "\n",
      "On pizzas scattered, in tapenade's art,\n",
      "A Mediterranean piece, a vital part.\n",
      "A symbol of peace, a culinary grace,\n",
      "The humble olive finds its modern place.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=short_config,\n",
    "    contents='Escribe un poema corto sobre la importancia de las aceitunas en la sociedad moderna.')\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZhDSLB6lqqB"
   },
   "source": [
    "Explora con tus propios prompts. Prueba un prompt con un límite de salida restrictivo y luego ajusta el prompt para trabajar dentro de ese límite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alx-WaAvir_9"
   },
   "source": [
    "### Temperatura\n",
    "\n",
    "La temperatura controla el grado de aleatoriedad en la selección de tokens. Las temperaturas más altas resultan en un mayor número de tokens candidatos de los cuales se selecciona el siguiente token de salida, y pueden producir resultados más diversos, mientras que las temperaturas más bajas tienen el efecto contrario, de modo que una temperatura de 0 resulta en una decodificación codiciosa, seleccionando el token más probable en cada paso.\n",
    "\n",
    "La temperatura no proporciona ninguna garantía de aleatoriedad, pero se puede usar para \"empujar\" la salida de alguna manera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:04:02.934124Z",
     "iopub.status.busy": "2025-03-31T10:04:02.933814Z",
     "iopub.status.idle": "2025-03-31T10:04:04.178735Z",
     "shell.execute_reply": "2025-03-31T10:04:04.177640Z",
     "shell.execute_reply.started": "2025-03-31T10:04:02.934091Z"
    },
    "id": "SHraGMzqnZqt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purple\n",
      " -------------------------\n",
      "Magenta\n",
      " -------------------------\n",
      "Cerulean\n",
      " -------------------------\n",
      "Magenta\n",
      " -------------------------\n",
      "Orange\n",
      " -------------------------\n"
     ]
    }
   ],
   "source": [
    "high_temp_config = types.GenerateContentConfig(temperature=2.0)\n",
    "\n",
    "\n",
    "for _ in range(5):\n",
    "  response = client.models.generate_content(\n",
    "      model='gemini-2.0-flash',\n",
    "      config=high_temp_config,\n",
    "      contents='Elige un color al azar... (responde en una sola palabra)')\n",
    "\n",
    "  if response.text:\n",
    "    print(response.text, '-' * 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3J4pCTuof7e"
   },
   "source": [
    "Ahora prueba el mismo prompt con la temperatura configurada a cero. Ten en cuenta que la salida no es completamente determinista, ya que otros parámetros afectan la selección de tokens, pero los resultados tienden a ser más estables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:04:04.181194Z",
     "iopub.status.busy": "2025-03-31T10:04:04.180253Z",
     "iopub.status.idle": "2025-03-31T10:04:05.389660Z",
     "shell.execute_reply": "2025-03-31T10:04:05.388259Z",
     "shell.execute_reply.started": "2025-03-31T10:04:04.181138Z"
    },
    "id": "clymkWv-PfUZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure\n",
      " -------------------------\n",
      "Azure\n",
      " -------------------------\n",
      "Azure\n",
      " -------------------------\n",
      "Azure\n",
      " -------------------------\n",
      "Azure\n",
      " -------------------------\n"
     ]
    }
   ],
   "source": [
    "low_temp_config = types.GenerateContentConfig(temperature=0.0)\n",
    "\n",
    "for _ in range(5):\n",
    "  response = client.models.generate_content(\n",
    "      model='gemini-2.0-flash',\n",
    "      config=low_temp_config,\n",
    "      contents='Elige un color al azar... (responde en una sola palabra)')\n",
    "\n",
    "  if response.text:\n",
    "    print(response.text, '-' * 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "St5nt3vzitsZ"
   },
   "source": [
    "### Top-P\n",
    "\n",
    "Al igual que la temperatura, el parámetro top-P también se utiliza para controlar la diversidad de la salida del modelo.\n",
    "\n",
    "Top-P define el umbral de probabilidad que, una vez superado acumulativamente, los tokens dejan de ser seleccionados como candidatos. Un top-P de 0 es típicamente equivalente a la decodificación codiciosa, y un top-P de 1 típicamente selecciona todos los tokens en el vocabulario del modelo.\n",
    "\n",
    "También puedes ver top-K referenciado en la literatura de LLM. Top-K no es configurable en la serie de modelos Gemini 2.0, pero se puede cambiar en modelos más antiguos. Top-K es un número entero positivo que define el número de tokens más probables de los cuales seleccionar el token de salida. Un top-K de 1 selecciona un solo token, realizando una decodificación codiciosa.\n",
    "\n",
    "\n",
    "Ejecuta este ejemplo varias veces, cambia la configuración y observa el cambio en la salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:04:05.391231Z",
     "iopub.status.busy": "2025-03-31T10:04:05.390910Z",
     "iopub.status.idle": "2025-03-31T10:04:10.684509Z",
     "shell.execute_reply": "2025-03-31T10:04:10.683377Z",
     "shell.execute_reply.started": "2025-03-31T10:04:05.391199Z"
    },
    "id": "lPlzpEavUV8F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whiskers twitched, emerald eyes narrowed. Jasper, a ginger tabby of discerning tastes (salmon pate for breakfast, sunbeams for naps), was bored. The predictable rhythm of lapping milk and batting at dust bunnies had lost its luster. He craved… adventure.\n",
      "\n",
      "His opportunity arrived with the morning mail. Mrs. Higgins, his beloved human, was distracted by a particularly lurid headline about a runaway llama. In the chaos, the back door swung ajar. Jasper, without a second thought, slipped through.\n",
      "\n",
      "The world outside was a cacophony of smells and sights. Grass, ticklish and unfamiliar, carpeted the earth. Birds, unlike the predictable chirping of the cage-bound canary inside, swooped and called with wild abandon.\n",
      "\n",
      "Jasper padded cautiously, his tail held high. He was a magnificent explorer, a fearless pioneer charting unknown territories. At least, that's what he told himself. The truth was, he was a little scared.\n",
      "\n",
      "He soon encountered Bartholomew, a grizzled, one-eared tomcat with a permanent scowl. Bartholomew held court on a discarded milk crate, surrounded by a gang of scruffy felines. This was clearly serious cat business.\n",
      "\n",
      "\"Well, well, well,\" Bartholomew rasped, his voice like sandpaper. \"Look what the cat dragged in. A fluffy housecat, come to admire our street cred?\"\n",
      "\n",
      "Jasper, puffed up with bravado (and a little bit of adrenaline), straightened his fur. \"I am here for adventure,\" he declared. \"Not to admire anyone's… cred.\"\n",
      "\n",
      "Bartholomew chuckled, a low rumble in his chest. \"Adventure? You think chasing butterflies is adventure? We're talking real adventure, kitten. The legend of the Whispering Warehouse.\"\n",
      "\n",
      "Jasper's ears pricked. \"The Whispering Warehouse?\"\n",
      "\n",
      "Bartholomew leaned closer, his breath smelling of fish scraps and secrets. \"They say it's filled with treasures, guarded by… well, that's the adventure, isn't it?\"\n",
      "\n",
      "And so, Jasper found himself embroiled in a quest. He joined Bartholomew and his ragtag crew, navigating alleyways, scaling fences, and dodging grumpy dogs. They faced challenges beyond his wildest housecat dreams – a ferocious chihuahua named Napoleon, a leaky sewer grate, and the agonizing wait for the garbage truck to pass.\n",
      "\n",
      "Finally, they reached the Whispering Warehouse, a towering structure with boarded-up windows and an air of eerie silence. The air hummed with an unknown energy. Bartholomew, despite his bravado, looked nervous.\n",
      "\n",
      "Inside, the warehouse was a labyrinth of forgotten goods. Dust motes danced in the dim light, illuminating piles of discarded furniture and forgotten toys. But there, in the center of the room, was a pile of… yarn.\n",
      "\n",
      "Mountains and mountains of yarn. In every color imaginable. Soft, fluffy, irresistible yarn.\n",
      "\n",
      "For Jasper, it was more valuable than gold. He launched himself into the pile, burying himself in the comforting texture. He purred with delight, batting and kneading, lost in a world of woolly bliss.\n",
      "\n",
      "Bartholomew and his crew stared in disbelief. \"This is it?\" Bartholomew croaked. \"All this for… yarn?\"\n",
      "\n",
      "Jasper, momentarily emerging from his yarn coma, shrugged. \"Adventure is what you make it,\" he said, before disappearing back into the fluffy abyss.\n",
      "\n",
      "Later, as the sun began to set, Jasper emerged from the warehouse, a single strand of vibrant blue yarn trailing from his whiskers. He bid farewell to Bartholomew and his crew, his heart full of gratitude and a strange, fuzzy sense of accomplishment.\n",
      "\n",
      "He slipped back through the ajar back door, unnoticed. Mrs. Higgins was still preoccupied with the llama. Jasper, exhausted but exhilarated, curled up in his favorite sunbeam.\n",
      "\n",
      "The adventure had changed him. He wasn't just a fluffy housecat anymore. He was Jasper, the Explorer, the Yarn Adventurer, the… Sleepy Cat. He yawned, tucked his paws under his chin, and drifted off to sleep, dreaming of mountains of yarn and the thrill of the unknown. He had a feeling this was just the beginning.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_config = types.GenerateContentConfig(\n",
    "    # Estos son los valores predeterminados para gemini-2.0-flash.\n",
    "    temperature=1.0,\n",
    "    top_p=0.95,\n",
    ")\n",
    "\n",
    "story_prompt = \"Eres un escritor creativo. Escribe una historia corta sobre un gato que se embarca en una aventura.\"\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=model_config,\n",
    "    contents=story_prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMrYs1koY6DX"
   },
   "source": [
    "## Creación de Prompts\n",
    "\n",
    "Esta sección contiene algunos prompts del capítulo para que los pruebes directamente en la API. Intenta cambiar el texto aquí para ver cómo se desempeña cada prompt con diferentes instrucciones, más ejemplos o cualquier otro cambio que se te ocurra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhj_tQidZJP7"
   },
   "source": [
    "### Zero-shot\n",
    "\n",
    "Los prompts zero-shot son prompts que describen la solicitud para el modelo directamente.\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/1gzKKgDHwkAvexG5Up0LMtl1-6jKMKe4g\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Abrir en AI Studio</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:04:10.686115Z",
     "iopub.status.busy": "2025-03-31T10:04:10.685842Z",
     "iopub.status.idle": "2025-03-31T10:05:18.675235Z",
     "shell.execute_reply": "2025-03-31T10:05:18.674173Z",
     "shell.execute_reply.started": "2025-03-31T10:04:10.686088Z"
    },
    "id": "1_t-cwnDZzbH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIVE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_config = types.GenerateContentConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=1,\n",
    "    max_output_tokens=5,\n",
    ")\n",
    "\n",
    "zero_shot_prompt = \"\"\"Clasifica las reseñas de películas como POSITIVAS, NEUTRALES o NEGATIVAS.\n",
    "Reseña: \"Her\" es un estudio perturbador que revela la dirección\n",
    "a la que se dirige la humanidad si se permite que la IA siga evolucionando,\n",
    "sin control. Ojalá hubiera más películas como esta obra maestra.\n",
    "Sentimiento: \"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=model_config,\n",
    "    contents=zero_shot_prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b5568bdeb11"
   },
   "source": [
    "#### Modo Enum\n",
    "\n",
    "Los modelos están entrenados para generar texto, y aunque los modelos Gemini 2.0 son excelentes para seguir instrucciones, otros modelos a veces pueden producir más texto del que deseas. En el ejemplo anterior, el modelo generará la etiqueta, pero a veces puede incluir una etiqueta \"Sentimiento\" precedente, y sin un límite de tokens de salida, también puede agregar texto explicativo después. Consulta [este prompt en AI Studio](https://aistudio.google.com/prompts/1gzKKgDHwkAvexG5Up0LMtl1-6jKMKe4g) para ver un ejemplo.\n",
    "\n",
    "La API de Gemini tiene una función de [modo Enum](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Enum.ipynb) que te permite restringir la salida a un conjunto fijo de valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:05:18.677294Z",
     "iopub.status.busy": "2025-03-31T10:05:18.676860Z",
     "iopub.status.idle": "2025-03-31T10:05:19.165251Z",
     "shell.execute_reply": "2025-03-31T10:05:19.164026Z",
     "shell.execute_reply.started": "2025-03-31T10:05:18.677246Z"
    },
    "id": "ad118a56c598"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "import enum\n",
    "\n",
    "class Sentiment(enum.Enum):\n",
    "    POSITIVE = \"positive\"\n",
    "    NEUTRAL = \"neutral\"\n",
    "    NEGATIVE = \"negative\"\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=types.GenerateContentConfig(\n",
    "        response_mime_type=\"text/x.enum\",\n",
    "        response_schema=Sentiment\n",
    "    ),\n",
    "    contents=zero_shot_prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al usar una salida restringida como un enum, el SDK de Python intentará convertir la respuesta de texto del modelo en un objeto de Python automáticamente. Se almacena en el campo `response.parsed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:05:19.167261Z",
     "iopub.status.busy": "2025-03-31T10:05:19.166944Z",
     "iopub.status.idle": "2025-03-31T10:05:19.173273Z",
     "shell.execute_reply": "2025-03-31T10:05:19.171617Z",
     "shell.execute_reply.started": "2025-03-31T10:05:19.167229Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment.POSITIVE\n",
      "<enum 'Sentiment'>\n"
     ]
    }
   ],
   "source": [
    "enum_response = response.parsed\n",
    "print(enum_response)\n",
    "print(type(enum_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0udiSwNbv45W"
   },
   "source": [
    "### One-shot y few-shot\n",
    "\n",
    "Proporcionar un ejemplo de la respuesta esperada se conoce como un prompt \"one-shot\". Cuando proporcionas múltiples ejemplos, es un prompt \"few-shot\".\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/1jjWkjUSoMXmLvMJ7IzADr_GxHPJVV2bg\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Abrir en AI Studio</a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:05:19.175570Z",
     "iopub.status.busy": "2025-03-31T10:05:19.175026Z",
     "iopub.status.idle": "2025-03-31T10:05:19.707971Z",
     "shell.execute_reply": "2025-03-31T10:05:19.706895Z",
     "shell.execute_reply.started": "2025-03-31T10:05:19.175463Z"
    },
    "id": "hd4mVUukwOKZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\"size\": \"large\",\n",
      "\"type\": \"normal\",\n",
      "\"ingredients\": [\"cheese\", \"pineapple\"]\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "few_shot_prompt = \"\"\"Parsea el pedido de pizza de un cliente en JSON válido:\n",
    "\n",
    "EJEMPLO:\n",
    "Quiero una pizza pequeña con queso, salsa de tomate y pepperoni.\n",
    "Respuesta JSON:\n",
    "```\n",
    "{\n",
    "\"size\": \"small\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"cheese\", \"tomato sauce\", \"pepperoni\"]\n",
    "}\n",
    "```\n",
    "\n",
    "EJEMPLO:\n",
    "¿Puedo obtener una pizza grande con salsa de tomate, albahaca y mozzarella?\n",
    "Respuesta JSON:\n",
    "```\n",
    "{\n",
    "\"size\": \"large\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
    "}\n",
    "```\n",
    "\n",
    "PEDIDO:\n",
    "\"\"\"\n",
    "\n",
    "customer_order = \"Dame una grande con queso y piña\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=250,\n",
    "    ),\n",
    "    contents=[few_shot_prompt, customer_order])\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "021293096f08"
   },
   "source": [
    "#### Modo JSON\n",
    "\n",
    "Para proporcionar control sobre el esquema y asegurarte de que solo recibas JSON (sin otro texto o markdown), puedes usar el [modo JSON](https://github.com/google-gemini/cookbook/blob/main/quickstarts/JSON_mode.ipynb) de la API de Gemini. Esto fuerza al modelo a restringir la decodificación, de modo que la selección de tokens se guíe por el esquema proporcionado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:05:19.709562Z",
     "iopub.status.busy": "2025-03-31T10:05:19.709202Z",
     "iopub.status.idle": "2025-03-31T10:05:20.328697Z",
     "shell.execute_reply": "2025-03-31T10:05:20.327445Z",
     "shell.execute_reply.started": "2025-03-31T10:05:19.709526Z"
    },
    "id": "50fbf0260912"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"size\": \"large\",\n",
      "  \"ingredients\": [\"apple\", \"chocolate\"],\n",
      "  \"type\": \"dessert pizza\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import typing_extensions as typing\n",
    "\n",
    "class PizzaOrder(typing.TypedDict):\n",
    "    size: str\n",
    "    ingredients: list[str]\n",
    "    type: str\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=0.1,\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=PizzaOrder,\n",
    "    ),\n",
    "    contents=\"¿Puedo tener una pizza de postre grande con manzana y chocolate?\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4a93e338e57c"
   },
   "source": [
    "### Cadena de Pensamiento (CoT)\n",
    "\n",
    "El prompting directo en LLMs puede devolver respuestas rápidamente y (en términos de uso de tokens de salida) de manera eficiente, pero pueden ser propensos a la alucinación. La respuesta puede \"parecer\" correcta (en términos de lenguaje y sintaxis) pero es incorrecta en términos de veracidad y razonamiento.\n",
    "\n",
    "El prompting de Cadena de Pensamiento es una técnica en la que instruyes al modelo para que genere pasos de razonamiento intermedios, y típicamente obtiene mejores resultados, especialmente cuando se combina con ejemplos few-shot. Vale la pena señalar que esta técnica no elimina completamente las alucinaciones, y que tiende a costar más ejecutarla, debido al aumento en el conteo de tokens.\n",
    "\n",
    "Modelos como la familia Gemini están entrenados para ser \"conversadores\" o \"reflexivos\" y proporcionarán pasos de razonamiento sin prompting, por lo que para este simple ejemplo puedes pedirle al modelo que sea más directo en el prompt para forzar una respuesta sin razonamiento. Intenta volver a ejecutar este paso si el modelo tiene suerte y obtiene la respuesta correcta en el primer intento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:05:20.330088Z",
     "iopub.status.busy": "2025-03-31T10:05:20.329790Z",
     "iopub.status.idle": "2025-03-31T10:05:20.741339Z",
     "shell.execute_reply": "2025-03-31T10:05:20.740335Z",
     "shell.execute_reply.started": "2025-03-31T10:05:20.330052Z"
    },
    "id": "5715555db1c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Cuando tenía 4 años, mi pareja tenía 3 veces mi edad. Ahora, tengo 20 años. ¿Cuántos años tiene mi pareja? Devuelve la respuesta directamente.\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e12b19677bfd"
   },
   "source": [
    "Ahora intenta el mismo enfoque, pero indica al modelo que \"piense paso a paso\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:05:20.742706Z",
     "iopub.status.busy": "2025-03-31T10:05:20.742394Z",
     "iopub.status.idle": "2025-03-31T10:05:21.530187Z",
     "shell.execute_reply": "2025-03-31T10:05:21.529098Z",
     "shell.execute_reply.started": "2025-03-31T10:05:20.742677Z"
    },
    "id": "ffd7536a481f"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's how to solve this:\n",
       "\n",
       "1. **Find the age difference:** When you were 4, your partner was 3 times your age, meaning they were 4 * 3 = 12 years old.\n",
       "\n",
       "2. **Calculate the age difference:** The age difference between you and your partner is 12 - 4 = 8 years.\n",
       "\n",
       "3. **Determine partner's current age:** Since the age difference remains constant, your partner is currently 20 + 8 = 28 years old.\n",
       "\n",
       "**Answer:** Your partner is currently 28 years old.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"Cuando tenía 4 años, mi pareja tenía 3 veces mi edad. Ahora,\n",
    "tengo 20 años. ¿Cuántos años tiene mi pareja? Pensemos paso a paso.\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=prompt)\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiLgBQJj0V53"
   },
   "source": [
    "### ReAct: Razonar y actuar\n",
    "\n",
    "En este ejemplo, ejecutarás un prompt ReAct directamente en la API de Gemini y realizarás los pasos de búsqueda tú mismo. Como este prompt sigue una estructura bien definida, existen marcos disponibles que envuelven el prompt en APIs más fáciles de usar que realizan llamadas a herramientas automáticamente, como el ejemplo de LangChain del documento técnico sobre \"Prompting\".\n",
    "\n",
    "Para probar esto con el motor de búsqueda de Wikipedia, consulta el [ejemplo de búsqueda en Wikipedia con ReAct](https://github.com/google-gemini/cookbook/blob/main/examples/Search_Wikipedia_using_ReAct.ipynb) en el libro de recetas.\n",
    "\n",
    "\n",
    "> Nota: El prompt y los ejemplos en contexto utilizados aquí son de [https://github.com/ysymyth/ReAct](https://github.com/ysymyth/ReAct) que se publica bajo una [licencia MIT](https://opensource.org/licenses/MIT), Copyright (c) 2023 Shunyu Yao.\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/18oo63Lwosd-bQ6Ay51uGogB3Wk3H8XMO\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Abrir en AI Studio</a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:05:21.532172Z",
     "iopub.status.busy": "2025-03-31T10:05:21.531769Z",
     "iopub.status.idle": "2025-03-31T10:05:21.539630Z",
     "shell.execute_reply": "2025-03-31T10:05:21.538374Z",
     "shell.execute_reply.started": "2025-03-31T10:05:21.532126Z"
    },
    "id": "cBgyNJ5z0VSs"
   },
   "outputs": [],
   "source": [
    "model_instructions = \"\"\"\n",
    "Resuelve una tarea de respuesta a preguntas con pasos intercalados de Pensamiento, Acción y Observación. El Pensamiento puede razonar sobre la situación actual,\n",
    "la Observación es comprender información relevante de la salida de una Acción y la Acción puede ser de tres tipos:\n",
    " (1) <search>entidad</search>, que busca la entidad exacta en Wikipedia y devuelve el primer párrafo si existe. Si no,\n",
    "     devolverá algunas entidades similares para buscar y puedes intentar buscar la información de esos temas.\n",
    " (2) <lookup>palabra clave</lookup>, que devuelve la siguiente oración que contiene la palabra clave en el contexto actual. Esto solo hace coincidencias exactas,\n",
    "     así que mantén tus búsquedas cortas.\n",
    " (3) <finish>respuesta</finish>, que devuelve la respuesta y finaliza la tarea.\n",
    "\"\"\"\n",
    "\n",
    "example1 = \"\"\"Pregunta\n",
    "El músico y satirista Allie Goertz escribió una canción sobre el personaje de \"Los Simpson\" Milhouse, ¿a quién nombró Matt Groening?\n",
    "\n",
    "Pensamiento 1\n",
    "La pregunta se simplifica a \"El personaje de Los Simpson Milhouse es nombrado después de quién\". Solo necesito buscar Milhouse y encontrar a quién está nombrado.\n",
    "\n",
    "Acción 1\n",
    "<search>Milhouse</search>\n",
    "\n",
    "Observación 1\n",
    "Milhouse Mussolini Van Houten es un personaje recurrente en la serie de televisión animada de Fox Los Simpson, con la voz de Pamela Hayden y creado por Matt Groening.\n",
    "\n",
    "Pensamiento 2\n",
    "El párrafo no dice a quién está nombrado Milhouse, tal vez pueda buscar \"nombrado después de\".\n",
    "\n",
    "Acción 2\n",
    "<lookup>nombrado después de</lookup>\n",
    "\n",
    "Observación 2\n",
    "Milhouse fue nombrado después del presidente de EE. UU. Richard Nixon, cuyo segundo nombre era Milhous.\n",
    "\n",
    "Pensamiento 3\n",
    "Milhouse fue nombrado después del presidente de EE. UU. Richard Nixon, así que la respuesta es Richard Nixon.\n",
    "\n",
    "Acción 3\n",
    "<finish>Richard Nixon</finish>\n",
    "\"\"\"\n",
    "\n",
    "example2 = \"\"\"Pregunta\n",
    "¿Cuál es el rango de elevación para el área en la que se extiende el sector oriental de la orogenia de Colorado?\n",
    "\n",
    "Pensamiento 1\n",
    "Necesito buscar la orogenia de Colorado, encontrar el área en la que se extiende el sector oriental de la orogenia de Colorado, luego encontrar el rango de elevación del área.\n",
    "\n",
    "Acción 1\n",
    "<search>orogenia de Colorado</search>\n",
    "\n",
    "Observación 1\n",
    "La orogenia de Colorado fue un episodio de construcción de montañas (una orogenia) en Colorado y áreas circundantes.\n",
    "\n",
    "Pensamiento 2\n",
    "No menciona el sector oriental. Así que necesito buscar el sector oriental.\n",
    "\n",
    "Acción 2\n",
    "<lookup>sector oriental</lookup>\n",
    "\n",
    "Observación 2\n",
    "El sector oriental se extiende hacia las Grandes Llanuras y se llama la orogenia de las Llanuras Centrales.\n",
    "\n",
    "Pensamiento 3\n",
    "El sector oriental de la orogenia de Colorado se extiende hacia las Grandes Llanuras. Así que necesito buscar las Grandes Llanuras y encontrar su rango de elevación.\n",
    "\n",
    "Acción 3\n",
    "<search>Grandes Llanuras</search>\n",
    "\n",
    "Observación 3\n",
    "Las Grandes Llanuras se refieren a una de dos regiones terrestres distintas\n",
    "\n",
    "Pensamiento 4\n",
    "Necesito buscar en su lugar las Grandes Llanuras (Estados Unidos).\n",
    "\n",
    "Acción 4\n",
    "<search>Grandes Llanuras (Estados Unidos)</search>\n",
    "\n",
    "Observación 4\n",
    "Las Grandes Llanuras son una subregión de las Grandes Llanuras. De este a oeste, las Grandes Llanuras se elevan en elevación desde alrededor de 1,800 a 7,000 pies (550 a 2,130m).\n",
    "\n",
    "Pensamiento 5\n",
    "Las Grandes Llanuras se elevan en elevación desde alrededor de 1,800 a 7,000 pies, así que la respuesta es 1,800 a 7,000 pies.\n",
    "\n",
    "Acción 5\n",
    "<finish>1,800 a 7,000 pies</finish>\n",
    "\"\"\"\n",
    "\n",
    "# Inventa más ejemplos tú mismo, o echa un vistazo a https://github.com/ysymyth/ReAct/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3wbfstjTgey"
   },
   "source": [
    "Para capturar un solo paso a la vez, mientras ignoras cualquier paso de Observación alucinado, usarás `stop_sequences` para finalizar el proceso de generación. Los pasos son `Pensamiento`, `Acción`, `Observación`, en ese orden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:05:21.541697Z",
     "iopub.status.busy": "2025-03-31T10:05:21.541288Z",
     "iopub.status.idle": "2025-03-31T10:05:22.176046Z",
     "shell.execute_reply": "2025-03-31T10:05:22.174821Z",
     "shell.execute_reply.started": "2025-03-31T10:05:21.541663Z"
    },
    "id": "8mxrXRkRTdXm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 1\n",
      "I need to find the transformers NLP paper and look for the author information. Then find the youngest author.\n",
      "\n",
      "Action 1\n",
      "<search>transformers NLP paper</search>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"Pregunta\n",
    "¿Quién fue el autor más joven listado en el artículo de transformers NLP?\n",
    "\"\"\"\n",
    "\n",
    "# Realizarás la Acción; así que genera hasta, pero no incluyendo, la Observación.\n",
    "react_config = types.GenerateContentConfig(\n",
    "    stop_sequences=[\"\\nObservación\"],\n",
    "    system_instruction=model_instructions + example1 + example2,\n",
    ")\n",
    "\n",
    "# Crea un chat que tenga las instrucciones del modelo y los ejemplos pre-cargados.\n",
    "react_chat = client.chats.create(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=react_config,\n",
    ")\n",
    "\n",
    "resp = react_chat.send_message(question)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aW2PIdLbVv3l"
   },
   "source": [
    "Ahora puedes realizar esta investigación tú mismo y proporcionarla de nuevo al modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:05:22.177839Z",
     "iopub.status.busy": "2025-03-31T10:05:22.177457Z",
     "iopub.status.idle": "2025-03-31T10:05:22.733581Z",
     "shell.execute_reply": "2025-03-31T10:05:22.732393Z",
     "shell.execute_reply.started": "2025-03-31T10:05:22.177792Z"
    },
    "id": "mLMc0DZaV9g2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 2\n",
      "I have found the paper, now I need to find the age of each author and find the youngest. This is difficult without searching each author individually. I will try to search for the ages of the authors.\n",
      "\n",
      "Action 2\n",
      "<search>Ashish Vaswani age</search>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "observation = \"\"\"Observación 1\n",
    "[1706.03762] Attention Is All You Need\n",
    "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
    "Proponemos una nueva arquitectura de red simple, el Transformer, basada únicamente en mecanismos de atención, prescindiendo completamente de la recurrencia y las convoluciones.\n",
    "\"\"\"\n",
    "resp = react_chat.send_message(observation)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bo0tzf4nX6dA"
   },
   "source": [
    "Este proceso se repite hasta que se alcanza la acción `<finish>`. Puedes continuar ejecutando esto tú mismo si lo deseas, o probar el [ejemplo de Wikipedia](https://github.com/google-gemini/cookbook/blob/main/examples/Search_Wikipedia_using_ReAct.ipynb) para ver un sistema ReAct completamente automatizado en acción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modo de pensamiento\n",
    "\n",
    "El modelo experimental Gemini Flash 2.0 \"Thinking\" ha sido entrenado para generar el \"proceso de pensamiento\" que el modelo atraviesa como parte de su respuesta. Como resultado, el modelo Flash Thinking es capaz de proporcionar respuestas con capacidades de razonamiento más fuertes.\n",
    "\n",
    "Usar un modelo de \"modo de pensamiento\" puede proporcionarte respuestas de alta calidad sin necesidad de prompting especializado como los enfoques anteriores. Una razón por la que esta técnica es efectiva es que induces al modelo a generar información relevante (\"lluvia de ideas\" o \"pensamientos\") que luego se utiliza como parte del contexto en el que se genera la respuesta final.\n",
    "\n",
    "Ten en cuenta que cuando usas la API, obtienes la respuesta final del modelo, pero los pensamientos no se capturan. Para ver los pensamientos intermedios, prueba el [modelo de modo de pensamiento en AI Studio](https://aistudio.google.com/prompts/new_chat?model=gemini-2.0-flash-thinking-exp-01-21).\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/1Z991SV7lZZZqioOiqIUPv9a9ix-ws4zk\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Abrir en AI Studio</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:05:22.738995Z",
     "iopub.status.busy": "2025-03-31T10:05:22.738665Z",
     "iopub.status.idle": "2025-03-31T10:05:29.680662Z",
     "shell.execute_reply": "2025-03-31T10:05:29.679463Z",
     "shell.execute_reply.started": "2025-03-31T10:05:22.738962Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the information available online, and considering typical academic career paths, **Aidan N. Gomez** appears to be the youngest author listed on the \"Attention is All You Need\" paper, which introduced the Transformer architecture.\n",
       "\n",
       "Here's why:\n",
       "\n",
       "* **Aidan N. Gomez** was an undergraduate student at the University of Oxford at the time the paper was written.  His LinkedIn profile indicates he graduated from Oxford in 2017, the same year the paper was published.  This typically places someone in their early 20s when publishing such a paper as an undergraduate.\n",
       "\n",
       "* The other authors were researchers at Google Brain, University of Toronto, and RWTH Aachen University, and many held PhDs or were in advanced stages of their research careers. This generally suggests they were older than an undergraduate student.\n",
       "\n",
       "While precise birthdates aren't readily available for all authors to definitively confirm age,  the academic stage of Aidan N. Gomez at the time of publication strongly suggests he was the youngest author on the paper.\n",
       "\n",
       "Therefore, the most likely answer is **Aidan N. Gomez**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "from IPython.display import Markdown, clear_output\n",
    "\n",
    "\n",
    "response = client.models.generate_content_stream(\n",
    "    model='gemini-2.0-flash-thinking-exp',\n",
    "    contents='¿Quién fue el autor más joven listado en el artículo de transformers NLP?',\n",
    ")\n",
    "\n",
    "buf = io.StringIO()\n",
    "for chunk in response:\n",
    "    buf.write(chunk.text)\n",
    "    # Muestra la respuesta a medida que se transmite\n",
    "    print(chunk.text, end='')\n",
    "\n",
    "# Y luego renderiza la respuesta final como markdown formateado.\n",
    "clear_output()\n",
    "Markdown(buf.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPiZ_eIIaVPt"
   },
   "source": [
    "## Creación de código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZinKamwXeR6C"
   },
   "source": [
    "### Generación de código\n",
    "\n",
    "La familia de modelos Gemini se puede utilizar para generar código, configuraciones y scripts. Generar código puede ser útil al aprender a programar, aprender un nuevo lenguaje o para generar rápidamente un primer borrador.\n",
    "\n",
    "Es importante tener en cuenta que, dado que los LLMs pueden cometer errores y pueden repetir datos de entrenamiento, es esencial leer y probar tu código primero, y cumplir con cualquier licencia relevante.\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/1YX71JGtzDjXQkgdes8bP6i3oH5lCRKxv\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Abrir en AI Studio</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:05:29.682517Z",
     "iopub.status.busy": "2025-03-31T10:05:29.682131Z",
     "iopub.status.idle": "2025-03-31T10:05:30.209135Z",
     "shell.execute_reply": "2025-03-31T10:05:30.207926Z",
     "shell.execute_reply.started": "2025-03-31T10:05:29.682479Z"
    },
    "id": "fOQP9pqmeUO1"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def factorial(n):\n",
       "  \"\"\"Calculate the factorial of a number.\"\"\"\n",
       "  if n == 0:\n",
       "    return 1\n",
       "  else:\n",
       "    return n * factorial(n-1)\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A los modelos Gemini les encanta hablar, así que ayuda especificar que se ciñan al código si eso\n",
    "# es todo lo que deseas.\n",
    "code_prompt = \"\"\"\n",
    "Escribe una función en Python para calcular el factorial de un número. Sin explicación, proporciona solo el código.\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=1024,\n",
    "    ),\n",
    "    contents=code_prompt)\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlBMWSFhgVRQ"
   },
   "source": [
    "### Ejecución de código\n",
    "\n",
    "La API de Gemini también puede ejecutar automáticamente el código generado y devolverá la salida.\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/11veFr_VYEwBWcLkhNLr-maCG0G8sS_7Z\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Abrir en AI Studio</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:05:30.211033Z",
     "iopub.status.busy": "2025-03-31T10:05:30.210623Z",
     "iopub.status.idle": "2025-03-31T10:05:32.465610Z",
     "shell.execute_reply": "2025-03-31T10:05:32.464035Z",
     "shell.execute_reply.started": "2025-03-31T10:05:30.210989Z"
    },
    "id": "jT3OfWYfhjRL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Okay, I can do that. First, I will generate the first 14 odd prime '\n",
      "         'numbers. Remember that a prime number is a number greater than 1 '\n",
      "         'that has only two divisors: 1 and itself. Also, note that 2 is the '\n",
      "         'only even prime number, so all other prime numbers are odd. After '\n",
      "         'generating these numbers, I will sum them.\\n'\n",
      "         '\\n'}\n",
      "-----\n",
      "{'executable_code': {'code': 'def is_prime(n):\\n'\n",
      "                             '    if n <= 1:\\n'\n",
      "                             '        return False\\n'\n",
      "                             '    if n <= 3:\\n'\n",
      "                             '        return True\\n'\n",
      "                             '    if n % 2 == 0 or n % 3 == 0:\\n'\n",
      "                             '        return False\\n'\n",
      "                             '    i = 5\\n'\n",
      "                             '    while i * i <= n:\\n'\n",
      "                             '        if n % i == 0 or n % (i + 2) == 0:\\n'\n",
      "                             '            return False\\n'\n",
      "                             '        i += 6\\n'\n",
      "                             '    return True\\n'\n",
      "                             '\\n'\n",
      "                             'primes = []\\n'\n",
      "                             'num = 3\\n'\n",
      "                             'while len(primes) < 14:\\n'\n",
      "                             '    if is_prime(num):\\n'\n",
      "                             '        primes.append(num)\\n'\n",
      "                             '    num += 2\\n'\n",
      "                             '\\n'\n",
      "                             \"print(f'{primes=}')\\n\"\n",
      "                             '\\n'\n",
      "                             'sum_of_primes = sum(primes)\\n'\n",
      "                             \"print(f'{sum_of_primes=}')\\n\",\n",
      "                     'language': 'PYTHON'}}\n",
      "-----\n",
      "{'code_execution_result': {'outcome': 'OUTCOME_OK',\n",
      "                           'output': 'primes=[3, 5, 7, 11, 13, 17, 19, 23, 29, '\n",
      "                                     '31, 37, 41, 43, 47]\\n'\n",
      "                                     'sum_of_primes=326\\n'}}\n",
      "-----\n",
      "{'text': 'The first 14 odd prime numbers are 3, 5, 7, 11, 13, 17, 19, 23, 29, '\n",
      "         '31, 37, 41, 43, and 47.\\n'\n",
      "         '\\n'\n",
      "         'The sum of these prime numbers is 326.\\n'}\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "config = types.GenerateContentConfig(\n",
    "    tools=[types.Tool(code_execution=types.ToolCodeExecution())],\n",
    ")\n",
    "\n",
    "code_exec_prompt = \"\"\"\n",
    "Genera los primeros 14 números primos impares, luego calcula su suma.\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=config,\n",
    "    contents=code_exec_prompt)\n",
    "\n",
    "for part in response.candidates[0].content.parts:\n",
    "  pprint(part.to_json_dict())\n",
    "  print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZspT1GSkjG6d"
   },
   "source": [
    "Esta respuesta contiene múltiples partes, incluyendo una parte de texto de apertura y cierre que representan respuestas regulares, una parte `executable_code` que representa el código generado y una parte `code_execution_result` que representa los resultados de ejecutar el código generado.\n",
    "\n",
    "Puedes explorarlas individualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:05:32.467952Z",
     "iopub.status.busy": "2025-03-31T10:05:32.467378Z",
     "iopub.status.idle": "2025-03-31T10:05:32.484683Z",
     "shell.execute_reply": "2025-03-31T10:05:32.483654Z",
     "shell.execute_reply.started": "2025-03-31T10:05:32.467898Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, I can do that. First, I will generate the first 14 odd prime numbers. Remember that a prime number is a number greater than 1 that has only two divisors: 1 and itself. Also, note that 2 is the only even prime number, so all other prime numbers are odd. After generating these numbers, I will sum them.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def is_prime(n):\n",
       "    if n <= 1:\n",
       "        return False\n",
       "    if n <= 3:\n",
       "        return True\n",
       "    if n % 2 == 0 or n % 3 == 0:\n",
       "        return False\n",
       "    i = 5\n",
       "    while i * i <= n:\n",
       "        if n % i == 0 or n % (i + 2) == 0:\n",
       "            return False\n",
       "        i += 6\n",
       "    return True\n",
       "\n",
       "primes = []\n",
       "num = 3\n",
       "while len(primes) < 14:\n",
       "    if is_prime(num):\n",
       "        primes.append(num)\n",
       "    num += 2\n",
       "\n",
       "print(f'{primes=}')\n",
       "\n",
       "sum_of_primes = sum(primes)\n",
       "print(f'{sum_of_primes=}')\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "primes=[3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\n",
       "sum_of_primes=326\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The first 14 odd prime numbers are 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, and 47.\n",
       "\n",
       "The sum of these prime numbers is 326.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for part in response.candidates[0].content.parts:\n",
    "    if part.text:\n",
    "        display(Markdown(part.text))\n",
    "    elif part.executable_code:\n",
    "        display(Markdown(f'```python\\n{part.executable_code.code}\\n```'))\n",
    "    elif part.code_execution_result:\n",
    "        if part.code_execution_result.outcome != 'OUTCOME_OK':\n",
    "            display(Markdown(f'## Estado {part.code_execution_result.outcome}'))\n",
    "\n",
    "        display(Markdown(f'```\\n{part.code_execution_result.output}\\n```'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gUX8QzCj4d5"
   },
   "source": [
    "### Explicación de código\n",
    "\n",
    "La familia de modelos Gemini también puede explicarte el código. En este ejemplo, pasas un [script de bash](https://github.com/magicmonty/bash-git-prompt) y haces algunas preguntas.\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/1N7LGzWzCYieyOf_7bAG4plrmkpDNmUyb\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Abrir en AI Studio</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:05:32.486266Z",
     "iopub.status.busy": "2025-03-31T10:05:32.485947Z",
     "iopub.status.idle": "2025-03-31T10:05:34.472037Z",
     "shell.execute_reply": "2025-03-31T10:05:34.471006Z",
     "shell.execute_reply.started": "2025-03-31T10:05:32.486236Z"
    },
    "id": "7_jPMMoxkIEb"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This file, `git-prompt.sh`, is a shell script designed to enhance your command-line prompt when working with Git repositories.  It provides information about the current Git status directly in your prompt, such as the branch name, the state of tracked/untracked files, and the divergence from the remote repository.\n",
       "\n",
       "**In essence, it's a cosmetic and productivity tool for developers using Git.**\n",
       "\n",
       "Here's a breakdown of why you might use it:\n",
       "\n",
       "*   **Git Status at a Glance:**  Instead of constantly running `git status`, the script displays key information (branch, modified files, etc.) directly in your prompt.\n",
       "*   **Improved Workflow:**  Knowing the Git status immediately helps you avoid mistakes and stay organized.\n",
       "*   **Customization:**  The script offers many options for customizing the prompt's appearance, including colors, symbols, and the information displayed.\n",
       "*   **Virtual Environment awareness**: The script also shows any virtual environments that you are working on, such as virtualenv, node virtualenv, and conda environments.\n",
       "\n",
       "In short, this file helps you create a more informative and visually appealing command-line prompt that integrates seamlessly with Git.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_contents = !curl https://raw.githubusercontent.com/magicmonty/bash-git-prompt/refs/heads/master/gitprompt.sh\n",
    "\n",
    "explain_prompt = f\"\"\"\n",
    "Por favor, explica qué hace este archivo a un nivel muy alto. ¿Qué es y por qué lo usaría?\n",
    "\n",
    "```\n",
    "{file_contents}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=explain_prompt)\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a8266d97ce5"
   },
   "source": [
    "## Aprende más\n",
    "\n",
    "Para aprender más sobre la creación de prompts en profundidad:\n",
    "\n",
    "* Consulta el documento técnico emitido con el contenido de hoy,\n",
    "* Prueba las aplicaciones listadas en la parte superior de este notebook ([TextFX](https://textfx.withgoogle.com/), [SQL Talk](https://sql-talk-r5gdynozbq-uc.a.run.app/) y [NotebookLM](https://notebooklm.google/)),\n",
    "* Lee la [Introducción a la creación de prompts](https://ai.google.dev/gemini-api/docs/prompting-intro) de la documentación de la API de Gemini,\n",
    "* Explora la [galería de prompts](https://ai.google.dev/gemini-api/prompts) de la API de Gemini y pruébalos en AI Studio,\n",
    "* Consulta el libro de recetas de la API de Gemini para [ejemplos inspiradores](https://github.com/google-gemini/cookbook/blob/main/examples/) y [inicios rápidos educativos](https://github.com/google-gemini/cookbook/blob/main/quickstarts/).\n",
    "\n",
    "Asegúrate de revisar los laboratorios de código del día 3 también, donde explorarás una creación de prompts más avanzada con ejecución de código.\n",
    "\n",
    "¡Y por favor comparte cualquier cosa emocionante que hayas probado en el Discord!\n",
    "\n",
    "*- [Mark McD](https://linktr.ee/markmcd)*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "day-1-prompting.ipynb",
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
