{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkxRSYjzA1oX"
   },
   "source": [
    "##### Copyright 2025 Google LLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2025-03-31T10:03:40.865454Z",
     "iopub.status.busy": "2025-03-31T10:03:40.865029Z",
     "iopub.status.idle": "2025-03-31T10:03:40.871949Z",
     "shell.execute_reply": "2025-03-31T10:03:40.870628Z",
     "shell.execute_reply.started": "2025-03-31T10:03:40.865391Z"
    },
    "id": "5u5OZ2ShA3BA",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# @title Licenciado bajo la Licencia Apache, Versi√≥n 2.0 (la \"Licencia\");\n",
    "# no puedes usar este archivo excepto en cumplimiento con la Licencia.\n",
    "# Puedes obtener una copia de la Licencia en\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# A menos que sea requerido por la ley aplicable o acordado por escrito, el software\n",
    "# distribuido bajo la Licencia se distribuye \"TAL CUAL\",\n",
    "# SIN GARANT√çAS O CONDICIONES DE NING√öN TIPO, ya sean expresas o impl√≠citas.\n",
    "# Consulta la Licencia para conocer el lenguaje espec√≠fico que rige los permisos y\n",
    "# limitaciones bajo la Licencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csNPnkuCobmG"
   },
   "source": [
    "# D√≠a 1 - Creaci√≥n de Prompts\n",
    "\n",
    "¬°Bienvenido al curso de 5 d√≠as de Generative AI en Kaggle!\n",
    "\n",
    "Este notebook te mostrar√° c√≥mo comenzar con la API de Gemini y te guiar√° a trav√©s de algunos ejemplos de prompts y t√©cnicas que tambi√©n puedes leer en el documento t√©cnico sobre Prompting. No necesitas leer el documento t√©cnico para usar este notebook, pero los documentos te dar√°n un contexto te√≥rico y antecedentes para complementar este notebook interactivo.\n",
    "\n",
    "\n",
    "## Antes de comenzar\n",
    "\n",
    "En este notebook, comenzar√°s a explorar la creaci√≥n de prompts utilizando el SDK de Python y AI Studio. Para inspirarte, podr√≠as disfrutar explorando algunas aplicaciones que se han creado utilizando la familia de modelos Gemini. Aqu√≠ hay algunas que nos gustan, y creemos que a ti tambi√©n te gustar√°n.\n",
    "\n",
    "* [TextFX](https://textfx.withgoogle.com/) es un conjunto de herramientas impulsadas por IA para raperos, creado en colaboraci√≥n con Lupe Fiasco,\n",
    "* [SQL Talk](https://sql-talk-r5gdynozbq-uc.a.run.app/) muestra c√≥mo puedes hablar directamente con una base de datos utilizando la API de Gemini,\n",
    "* [NotebookLM](https://notebooklm.google/) utiliza modelos Gemini para construir tu propio asistente de investigaci√≥n personal impulsado por IA.\n",
    "\n",
    "\n",
    "## Para obtener ayuda\n",
    "\n",
    "**Los problemas comunes est√°n cubiertos en la [gu√≠a de preguntas frecuentes y soluci√≥n de problemas](https://www.kaggle.com/code/markishere/day-0-troubleshooting-and-faqs).**\n",
    "\n",
    "## ¬°Novedades de Gemini 2.0!\n",
    "\n",
    "Este material del curso se lanz√≥ por primera vez en noviembre de 2024. El espacio de IA y LLM est√° avanzando incre√≠blemente r√°pido, por lo que hemos realizado algunas actualizaciones para usar los √∫ltimos modelos y capacidades.\n",
    "\n",
    "* Estos laboratorios de c√≥digo se han actualizado para usar la familia de modelos Gemini 2.0.\n",
    "* El SDK de Python se ha actualizado de `google-generativeai` al nuevo y unificado SDK [`google-genai`](https://pypi.org/project/google-genai).\n",
    "  * Este nuevo SDK funciona tanto con la API de desarrolladores de Gemini como con Google Cloud Vertex AI, y cambiar entre ellos es [tan simple como cambiar algunos campos](https://pypi.org/project/google-genai/#:~:text=.Client%28%29-,API%20Selection,-By%20default%2C%20the).\n",
    "* Se han agregado nuevas capacidades de modelo a los laboratorios de c√≥digo relevantes, como el \"modo de pensamiento\" en este laboratorio.\n",
    "* El D√≠a 1 incluye un nuevo [laboratorio de evaluaci√≥n](https://www.kaggle.com/code/markishere/day-1-evaluation-and-structured-output)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f00ea7c99e44"
   },
   "source": [
    "## Comenzar con los notebooks de Kaggle\n",
    "\n",
    "Si es la primera vez que usas un notebook de Kaggle, ¬°bienvenido! Puedes leer sobre c√≥mo usar los notebooks de Kaggle [en la documentaci√≥n](https://www.kaggle.com/docs/notebooks).\n",
    "\n",
    "Primero, necesitar√°s verificar tu cuenta por tel√©fono en kaggle.com/settings.\n",
    "\n",
    "![](https://storage.googleapis.com/kaggle-media/Images/5dgai_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ejecutar este notebook, as√≠ como los dem√°s en este curso, necesitar√°s hacer una copia o bifurcar el notebook. Busca el bot√≥n `Copy and Edit` en la parte superior derecha y **haz clic en √©l** para hacer una copia privada editable del notebook. Deber√≠a verse como este:\n",
    "\n",
    "![Bot√≥n Copy and Edit](https://storage.googleapis.com/kaggle-media/Images/5gdai_sc_1.png)\n",
    "\n",
    "Tu copia ahora tendr√° un bot√≥n ‚ñ∂Ô∏è **Run** junto a cada celda de c√≥digo que puedes presionar para ejecutar esa celda. Se espera que estos notebooks se ejecuten en orden de arriba hacia abajo, pero se te anima a agregar nuevas celdas, ejecutar tu propio c√≥digo y explorar. Si te quedas atascado, puedes intentar la opci√≥n `Factory reset` en el men√∫ `Run`, o volver al notebook original y hacer una nueva copia.\n",
    "\n",
    "![Bot√≥n Run cell](https://storage.googleapis.com/kaggle-media/Images/5gdai_sc_2.png)\n",
    "\n",
    "### ¬øProblemas?\n",
    "\n",
    "Si tienes alg√∫n problema, dir√≠gete al [Discord de Kaggle](https://discord.com/invite/kaggle), encuentra el canal [`#5dgai-q-and-a`](https://discord.com/channels/1101210829807956100/1303438695143178251) y pide ayuda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExkOXcPxtTb5"
   },
   "source": [
    "## Comenzar con la API de Gemini\n",
    "\n",
    "Todos los ejercicios en este notebook utilizar√°n la [API de Gemini](https://ai.google.dev/gemini-api/) a trav√©s del [SDK de Python](https://pypi.org/project/google-genai/). Cada uno de estos prompts se puede acceder directamente en [Google AI Studio](https://aistudio.google.com/) tambi√©n, por lo que si prefieres usar una interfaz web y omitir el c√≥digo para esta actividad, busca el enlace <img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> AI Studio en cada prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuaci√≥n, necesitar√°s agregar tu clave API a tu Notebook de Kaggle como un Secreto de Usuario de Kaggle.\n",
    "\n",
    "![](https://storage.googleapis.com/kaggle-media/Images/5dgai_1.png)\n",
    "![](https://storage.googleapis.com/kaggle-media/Images/5dgai_2.png)\n",
    "![](https://storage.googleapis.com/kaggle-media/Images/5dgai_3.png)\n",
    "![](https://storage.googleapis.com/kaggle-media/Images/5dgai_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAjUV3BsvFXQ"
   },
   "source": [
    "### Instalar el SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:03:40.874310Z",
     "iopub.status.busy": "2025-03-31T10:03:40.873953Z",
     "iopub.status.idle": "2025-03-31T10:03:53.209301Z",
     "shell.execute_reply": "2025-03-31T10:03:53.207859Z",
     "shell.execute_reply.started": "2025-03-31T10:03:40.874270Z"
    },
    "id": "NzwzJFU9LqkJ"
   },
   "outputs": [],
   "source": [
    "!pip uninstall -qqy jupyterlab  # Remove unused packages from Kaggle's base image that conflict\n",
    "!pip install -U -q \"google-genai==1.7.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importa el SDK y algunos ayudantes para renderizar la salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:03:53.211769Z",
     "iopub.status.busy": "2025-03-31T10:03:53.211256Z",
     "iopub.status.idle": "2025-03-31T10:03:54.638132Z",
     "shell.execute_reply": "2025-03-31T10:03:54.636956Z",
     "shell.execute_reply.started": "2025-03-31T10:03:53.211723Z"
    },
    "id": "5DwxYIRavMST"
   },
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "from IPython.display import HTML, Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configura un ayudante de reintento. Esto te permite \"Ejecutar todo\" sin preocuparte por la cuota por minuto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:03:54.640904Z",
     "iopub.status.busy": "2025-03-31T10:03:54.640394Z",
     "iopub.status.idle": "2025-03-31T10:03:54.845515Z",
     "shell.execute_reply": "2025-03-31T10:03:54.844479Z",
     "shell.execute_reply.started": "2025-03-31T10:03:54.640869Z"
    }
   },
   "outputs": [],
   "source": [
    "from google.api_core import retry\n",
    "\n",
    "\n",
    "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
    "\n",
    "genai.models.Models.generate_content = retry.Retry(\n",
    "    predicate=is_retriable)(genai.models.Models.generate_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNEt2BCOvOJ1"
   },
   "source": [
    "### Configura tu clave API\n",
    "\n",
    "Para ejecutar la siguiente celda, tu clave API debe estar almacenada en un [secreto de Kaggle](https://www.kaggle.com/discussions/product-feedback/114053) llamado `GOOGLE_API_KEY`.\n",
    "\n",
    "Si a√∫n no tienes una clave API, puedes obtener una en [AI Studio](https://aistudio.google.com/app/apikey). Puedes encontrar [instrucciones detalladas en la documentaci√≥n](https://ai.google.dev/gemini-api/docs/api-key).\n",
    "\n",
    "Para hacer que la clave est√© disponible a trav√©s de secretos de Kaggle, elige `Secrets` en el men√∫ `Add-ons` y sigue las instrucciones para agregar tu clave o habilitarla para este notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:03:54.847270Z",
     "iopub.status.busy": "2025-03-31T10:03:54.846853Z",
     "iopub.status.idle": "2025-03-31T10:03:55.056841Z",
     "shell.execute_reply": "2025-03-31T10:03:55.055686Z",
     "shell.execute_reply.started": "2025-03-31T10:03:54.847240Z"
    },
    "id": "SHl0bkPCvayd"
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4e720472fd86"
   },
   "source": [
    "Si recibiste una respuesta de error del tipo `No user secrets exist for kernel id ...`, entonces necesitas agregar tu clave API a trav√©s de `Add-ons`, `Secrets` **y** habilitarla.\n",
    "\n",
    "![Captura de pantalla de la casilla de verificaci√≥n para habilitar el secreto GOOGLE_API_KEY](https://storage.googleapis.com/kaggle-media/Images/5gdai_sc_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_YXCYIKvyZJ"
   },
   "source": [
    "### Ejecuta tu primer prompt\n",
    "\n",
    "En este paso, probar√°s que tu clave API est√© configurada correctamente haciendo una solicitud.\n",
    "\n",
    "El SDK de Python utiliza un [`Client` object](https://googleapis.github.io/python-genai/genai.html#genai.client.Client) para hacer solicitudes a la API. El cliente te permite controlar qu√© back-end usar (entre la API de Gemini y Vertex AI) y maneja la autenticaci√≥n (la clave API).\n",
    "\n",
    "El modelo `gemini-2.0-flash` ha sido seleccionado aqu√≠.\n",
    "\n",
    "**Nota**: Si ves un `TransportError` en este paso, es posible que necesites **üîÅ Factory reset** el notebook una vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:03:55.058716Z",
     "iopub.status.busy": "2025-03-31T10:03:55.058250Z",
     "iopub.status.idle": "2025-03-31T10:03:57.886958Z",
     "shell.execute_reply": "2025-03-31T10:03:57.885798Z",
     "shell.execute_reply.started": "2025-03-31T10:03:55.058668Z"
    },
    "id": "BV1o0PmcvyJF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, imagine you have a really, REALLY smart robot. But instead of being made of metal and wires, this smartness lives inside a computer!\n",
      "\n",
      "This smartness is called Artificial Intelligence, or AI for short.\n",
      "\n",
      "Think of it like this:\n",
      "\n",
      "*   **A puppy learning tricks:** You teach a puppy by showing it what to do and rewarding it when it does it right. AI learns in a similar way, but with lots and lots of examples and by getting \"rewards\" for making the right choices.\n",
      "*   **A super-powered calculator:** A calculator can do math really fast. AI is like a super-powered calculator that can do lots more than just math! It can see pictures, understand words, and even play games.\n",
      "*   **Playing hide-and-seek:** If you play hide-and-seek a lot, you get better at finding good hiding spots. AI is the same. The more it \"plays\" (or works), the better it gets at things!\n",
      "\n",
      "**So, what can AI do?**\n",
      "\n",
      "*   **Help you find videos you like:** Have you ever noticed that YouTube suggests videos you might enjoy? That's AI! It's learned what kind of videos you watch and tries to guess what you'll like next.\n",
      "*   **Help you spell words:** When you're typing and you misspell a word, sometimes your computer fixes it. That's AI helping you!\n",
      "*   **Help doctors find diseases:** AI can look at X-rays and other medical images to help doctors find problems that might be hard to see with their own eyes.\n",
      "*   **Drive cars:** Some cars are starting to drive themselves using AI! They can see the road, other cars, and even pedestrians.\n",
      "\n",
      "**AI is still learning and growing, just like you!** It's a tool that can help us do amazing things, but it's important to remember that it's not perfect. We still need people to help it learn and make sure it's used for good.\n",
      "\n",
      "Does that make sense? Do you have any other questions about AI?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=\"Expl√≠came la IA como si fuera un ni√±o.\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f60ed9d8ae41"
   },
   "source": [
    "La respuesta a menudo viene en formato markdown, que puedes renderizar directamente en este notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:03:57.888876Z",
     "iopub.status.busy": "2025-03-31T10:03:57.888544Z",
     "iopub.status.idle": "2025-03-31T10:03:57.897544Z",
     "shell.execute_reply": "2025-03-31T10:03:57.896275Z",
     "shell.execute_reply.started": "2025-03-31T10:03:57.888845Z"
    },
    "id": "c933e5e460a5"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, imagine you have a really, REALLY smart robot. But instead of being made of metal and wires, this smartness lives inside a computer!\n",
       "\n",
       "This smartness is called Artificial Intelligence, or AI for short.\n",
       "\n",
       "Think of it like this:\n",
       "\n",
       "*   **A puppy learning tricks:** You teach a puppy by showing it what to do and rewarding it when it does it right. AI learns in a similar way, but with lots and lots of examples and by getting \"rewards\" for making the right choices.\n",
       "*   **A super-powered calculator:** A calculator can do math really fast. AI is like a super-powered calculator that can do lots more than just math! It can see pictures, understand words, and even play games.\n",
       "*   **Playing hide-and-seek:** If you play hide-and-seek a lot, you get better at finding good hiding spots. AI is the same. The more it \"plays\" (or works), the better it gets at things!\n",
       "\n",
       "**So, what can AI do?**\n",
       "\n",
       "*   **Help you find videos you like:** Have you ever noticed that YouTube suggests videos you might enjoy? That's AI! It's learned what kind of videos you watch and tries to guess what you'll like next.\n",
       "*   **Help you spell words:** When you're typing and you misspell a word, sometimes your computer fixes it. That's AI helping you!\n",
       "*   **Help doctors find diseases:** AI can look at X-rays and other medical images to help doctors find problems that might be hard to see with their own eyes.\n",
       "*   **Drive cars:** Some cars are starting to drive themselves using AI! They can see the road, other cars, and even pedestrians.\n",
       "\n",
       "**AI is still learning and growing, just like you!** It's a tool that can help us do amazing things, but it's important to remember that it's not perfect. We still need people to help it learn and make sure it's used for good.\n",
       "\n",
       "Does that make sense? Do you have any other questions about AI?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byx0pT9ZMW2Q"
   },
   "source": [
    "### Inicia un chat\n",
    "\n",
    "El ejemplo anterior utiliza una estructura de un solo turno, entrada de texto/salida de texto, pero tambi√©n puedes configurar una estructura de chat de varios turnos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:03:57.899193Z",
     "iopub.status.busy": "2025-03-31T10:03:57.898888Z",
     "iopub.status.idle": "2025-03-31T10:03:58.463778Z",
     "shell.execute_reply": "2025-03-31T10:03:58.462774Z",
     "shell.execute_reply.started": "2025-03-31T10:03:57.899163Z"
    },
    "id": "lV_S5ZL5MidD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice to meet you, Zlork! It's great to have you here. How can I help you today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat = client.chats.create(model='gemini-2.0-flash', history=[])\n",
    "response = chat.send_message('¬°Hola! Mi nombre es Zlork.')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:03:58.465802Z",
     "iopub.status.busy": "2025-03-31T10:03:58.465343Z",
     "iopub.status.idle": "2025-03-31T10:04:00.174979Z",
     "shell.execute_reply": "2025-03-31T10:04:00.173771Z",
     "shell.execute_reply.started": "2025-03-31T10:03:58.465753Z"
    },
    "id": "7b0372c3c64a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's something interesting about dinosaurs:\n",
      "\n",
      "**There's evidence that some dinosaurs were brightly colored, possibly with iridescent feathers, and even different colors within the same species, similar to modern birds!**\n",
      "\n",
      "For a long time, scientists believed dinosaurs were largely drab in color, like reptiles. However, through analyzing fossilized feathers and scales, and even the microscopic structures called melanosomes (which contain pigment), researchers have been able to infer the colors of some dinosaurs.\n",
      "\n",
      "This has led to the discovery that some dinosaurs might have had brilliant hues, stripes, spots, and even iridescent sheens like a hummingbird. The *Microraptor*, for example, is thought to have had iridescent black feathers. And the *Sinosauropteryx* is believed to have had reddish-brown stripes.\n",
      "\n",
      "This discovery changes our understanding of dinosaur behavior, suggesting that color may have played a role in camouflage, communication, and courtship, just like it does in modern birds.\n",
      "\n",
      "So, the next time you picture a dinosaur, don't just think of a giant, green lizard. Imagine it possibly strutting around with vibrant, eye-catching colors!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message('¬øPuedes decirme algo interesante sobre los dinosaurios?')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mientras tengas el objeto `chat` activo, el estado de la conversaci√≥n\n",
    "persiste. Confirma eso preguntando si recuerda el nombre del usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:04:00.178763Z",
     "iopub.status.busy": "2025-03-31T10:04:00.178357Z",
     "iopub.status.idle": "2025-03-31T10:04:00.597558Z",
     "shell.execute_reply": "2025-03-31T10:04:00.596167Z",
     "shell.execute_reply.started": "2025-03-31T10:04:00.178728Z"
    },
    "id": "d3f9591392a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, your name is Zlork.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message('¬øRecuerdas cu√°l es mi nombre?')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KAjpr1200sW"
   },
   "source": [
    "### Elige un modelo\n",
    "\n",
    "La API de Gemini proporciona acceso a varios modelos de la familia de modelos Gemini. Lee sobre los modelos disponibles y sus capacidades en la [p√°gina de descripci√≥n general del modelo](https://ai.google.dev/gemini-api/docs/models/gemini).\n",
    "\n",
    "En este paso, usar√°s la API para listar todos los modelos disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:04:00.599723Z",
     "iopub.status.busy": "2025-03-31T10:04:00.599246Z",
     "iopub.status.idle": "2025-03-31T10:04:00.631963Z",
     "shell.execute_reply": "2025-03-31T10:04:00.630701Z",
     "shell.execute_reply.started": "2025-03-31T10:04:00.599654Z"
    },
    "id": "uUUZa2uq2jDm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001\n",
      "models/text-bison-001\n",
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/gemini-2.5-pro-exp-03-25\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/learnlm-1.5-pro-experimental\n",
      "models/gemma-3-27b-it\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/aqa\n",
      "models/imagen-3.0-generate-002\n"
     ]
    }
   ],
   "source": [
    "for model in client.models.list():\n",
    "  print(model.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rN49kSI54R1v"
   },
   "source": [
    "La respuesta de [`models.list`](https://ai.google.dev/api/models#method:-models.list) tambi√©n devuelve informaci√≥n adicional sobre las capacidades del modelo, como los l√≠mites de tokens y los par√°metros compatibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:04:00.633866Z",
     "iopub.status.busy": "2025-03-31T10:04:00.633411Z",
     "iopub.status.idle": "2025-03-31T10:04:00.665926Z",
     "shell.execute_reply": "2025-03-31T10:04:00.664681Z",
     "shell.execute_reply.started": "2025-03-31T10:04:00.633819Z"
    },
    "id": "k7JJ1K6j4Rl8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': 'Gemini 2.0 Flash',\n",
      " 'display_name': 'Gemini 2.0 Flash',\n",
      " 'input_token_limit': 1048576,\n",
      " 'name': 'models/gemini-2.0-flash',\n",
      " 'output_token_limit': 8192,\n",
      " 'supported_actions': ['generateContent', 'countTokens'],\n",
      " 'tuned_model_info': {},\n",
      " 'version': '2.0'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for model in client.models.list():\n",
    "  if model.name == 'models/gemini-2.0-flash':\n",
    "    pprint(model.to_json_dict())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rU_UBlZdooM"
   },
   "source": [
    "## Explora los par√°metros de generaci√≥n\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7NfEizeipbW"
   },
   "source": [
    "### Longitud de salida\n",
    "\n",
    "Al generar texto con un LLM, la longitud de la salida afecta el costo y el rendimiento. Generar m√°s tokens aumenta la computaci√≥n, lo que lleva a un mayor consumo de energ√≠a, latencia y costo.\n",
    "\n",
    "Para detener el modelo de generar tokens m√°s all√° de un l√≠mite, puedes especificar el par√°metro `max_output_tokens` al usar la API de Gemini. Especificar este par√°metro no influye en la generaci√≥n de los tokens de salida, por lo que la salida no se volver√° m√°s concisa estil√≠stica o textualmente, pero dejar√° de generar tokens una vez que se alcance la longitud especificada. Puede ser necesario el dise√±o de prompts para generar una salida m√°s completa para tu l√≠mite dado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:04:00.667882Z",
     "iopub.status.busy": "2025-03-31T10:04:00.667458Z",
     "iopub.status.idle": "2025-03-31T10:04:02.147176Z",
     "shell.execute_reply": "2025-03-31T10:04:02.145805Z",
     "shell.execute_reply.started": "2025-03-31T10:04:00.667834Z"
    },
    "id": "qVf23JsIi9ma"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## The Humble Olive: A Cornerstone of Modern Society\n",
      "\n",
      "The olive, a small, often brine-soaked fruit, may seem insignificant amidst the complexities of modern society. Yet, beneath its unassuming exterior lies a profound importance, extending far beyond its culinary applications. From its historical roots to its contributions to health, economy, and culture, the olive tree and its fruit have woven themselves into the fabric of modern life, demonstrating an enduring relevance that belies its perceived simplicity.\n",
      "\n",
      "The story of the olive is deeply intertwined with the history of civilization. Originating in the Mediterranean basin thousands of years ago, the olive tree, *Olea europaea*, quickly became a cornerstone of ancient economies and societies. Olive oil was not merely a foodstuff; it was a source of fuel for lamps, a component of religious rituals, a base for perfumes and medicines, and a symbol of peace and prosperity. Ancient Greece valued the olive tree so highly that it was considered sacred to the goddess Athena, and victors in\n"
     ]
    }
   ],
   "source": [
    "from google.genai import types\n",
    "\n",
    "short_config = types.GenerateContentConfig(max_output_tokens=200)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=short_config,\n",
    "    contents='Escribe un ensayo de 1000 palabras sobre la importancia de las aceitunas en la sociedad moderna.')\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:04:02.149272Z",
     "iopub.status.busy": "2025-03-31T10:04:02.148834Z",
     "iopub.status.idle": "2025-03-31T10:04:02.932445Z",
     "shell.execute_reply": "2025-03-31T10:04:02.931442Z",
     "shell.execute_reply.started": "2025-03-31T10:04:02.149225Z"
    },
    "id": "W-3kR2F5kdMR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From groves sun-drenched and ancient roots,\n",
      "The olive's fruit, a bounty shoots.\n",
      "Pressed into oil, a golden stream,\n",
      "Nourishing bodies, fulfilling dream.\n",
      "\n",
      "On pizzas scattered, in tapenade's art,\n",
      "A Mediterranean piece, a vital part.\n",
      "A symbol of peace, a culinary grace,\n",
      "The humble olive finds its modern place.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=short_config,\n",
    "    contents='Escribe un poema corto sobre la importancia de las aceitunas en la sociedad moderna.')\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZhDSLB6lqqB"
   },
   "source": [
    "Explora con tus propios prompts. Prueba un prompt con un l√≠mite de salida restrictivo y luego ajusta el prompt para trabajar dentro de ese l√≠mite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alx-WaAvir_9"
   },
   "source": [
    "### Temperatura\n",
    "\n",
    "La temperatura controla el grado de aleatoriedad en la selecci√≥n de tokens. Las temperaturas m√°s altas resultan en un mayor n√∫mero de tokens candidatos de los cuales se selecciona el siguiente token de salida, y pueden producir resultados m√°s diversos, mientras que las temperaturas m√°s bajas tienen el efecto contrario, de modo que una temperatura de 0 resulta en una decodificaci√≥n codiciosa, seleccionando el token m√°s probable en cada paso.\n",
    "\n",
    "La temperatura no proporciona ninguna garant√≠a de aleatoriedad, pero se puede usar para \"empujar\" la salida de alguna manera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:04:02.934124Z",
     "iopub.status.busy": "2025-03-31T10:04:02.933814Z",
     "iopub.status.idle": "2025-03-31T10:04:04.178735Z",
     "shell.execute_reply": "2025-03-31T10:04:04.177640Z",
     "shell.execute_reply.started": "2025-03-31T10:04:02.934091Z"
    },
    "id": "SHraGMzqnZqt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purple\n",
      " -------------------------\n",
      "Magenta\n",
      " -------------------------\n",
      "Cerulean\n",
      " -------------------------\n",
      "Magenta\n",
      " -------------------------\n",
      "Orange\n",
      " -------------------------\n"
     ]
    }
   ],
   "source": [
    "high_temp_config = types.GenerateContentConfig(temperature=2.0)\n",
    "\n",
    "\n",
    "for _ in range(5):\n",
    "  response = client.models.generate_content(\n",
    "      model='gemini-2.0-flash',\n",
    "      config=high_temp_config,\n",
    "      contents='Elige un color al azar... (responde en una sola palabra)')\n",
    "\n",
    "  if response.text:\n",
    "    print(response.text, '-' * 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3J4pCTuof7e"
   },
   "source": [
    "Ahora prueba el mismo prompt con la temperatura configurada a cero. Ten en cuenta que la salida no es completamente determinista, ya que otros par√°metros afectan la selecci√≥n de tokens, pero los resultados tienden a ser m√°s estables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:04:04.181194Z",
     "iopub.status.busy": "2025-03-31T10:04:04.180253Z",
     "iopub.status.idle": "2025-03-31T10:04:05.389660Z",
     "shell.execute_reply": "2025-03-31T10:04:05.388259Z",
     "shell.execute_reply.started": "2025-03-31T10:04:04.181138Z"
    },
    "id": "clymkWv-PfUZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure\n",
      " -------------------------\n",
      "Azure\n",
      " -------------------------\n",
      "Azure\n",
      " -------------------------\n",
      "Azure\n",
      " -------------------------\n",
      "Azure\n",
      " -------------------------\n"
     ]
    }
   ],
   "source": [
    "low_temp_config = types.GenerateContentConfig(temperature=0.0)\n",
    "\n",
    "for _ in range(5):\n",
    "  response = client.models.generate_content(\n",
    "      model='gemini-2.0-flash',\n",
    "      config=low_temp_config,\n",
    "      contents='Elige un color al azar... (responde en una sola palabra)')\n",
    "\n",
    "  if response.text:\n",
    "    print(response.text, '-' * 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "St5nt3vzitsZ"
   },
   "source": [
    "### Top-P\n",
    "\n",
    "Al igual que la temperatura, el par√°metro top-P tambi√©n se utiliza para controlar la diversidad de la salida del modelo.\n",
    "\n",
    "Top-P define el umbral de probabilidad que, una vez superado acumulativamente, los tokens dejan de ser seleccionados como candidatos. Un top-P de 0 es t√≠picamente equivalente a la decodificaci√≥n codiciosa, y un top-P de 1 t√≠picamente selecciona todos los tokens en el vocabulario del modelo.\n",
    "\n",
    "Tambi√©n puedes ver top-K referenciado en la literatura de LLM. Top-K no es configurable en la serie de modelos Gemini 2.0, pero se puede cambiar en modelos m√°s antiguos. Top-K es un n√∫mero entero positivo que define el n√∫mero de tokens m√°s probables de los cuales seleccionar el token de salida. Un top-K de 1 selecciona un solo token, realizando una decodificaci√≥n codiciosa.\n",
    "\n",
    "\n",
    "Ejecuta este ejemplo varias veces, cambia la configuraci√≥n y observa el cambio en la salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:04:05.391231Z",
     "iopub.status.busy": "2025-03-31T10:04:05.390910Z",
     "iopub.status.idle": "2025-03-31T10:04:10.684509Z",
     "shell.execute_reply": "2025-03-31T10:04:10.683377Z",
     "shell.execute_reply.started": "2025-03-31T10:04:05.391199Z"
    },
    "id": "lPlzpEavUV8F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whiskers twitched, emerald eyes narrowed. Jasper, a ginger tabby of discerning tastes (salmon pate for breakfast, sunbeams for naps), was bored. The predictable rhythm of lapping milk and batting at dust bunnies had lost its luster. He craved‚Ä¶ adventure.\n",
      "\n",
      "His opportunity arrived with the morning mail. Mrs. Higgins, his beloved human, was distracted by a particularly lurid headline about a runaway llama. In the chaos, the back door swung ajar. Jasper, without a second thought, slipped through.\n",
      "\n",
      "The world outside was a cacophony of smells and sights. Grass, ticklish and unfamiliar, carpeted the earth. Birds, unlike the predictable chirping of the cage-bound canary inside, swooped and called with wild abandon.\n",
      "\n",
      "Jasper padded cautiously, his tail held high. He was a magnificent explorer, a fearless pioneer charting unknown territories. At least, that's what he told himself. The truth was, he was a little scared.\n",
      "\n",
      "He soon encountered Bartholomew, a grizzled, one-eared tomcat with a permanent scowl. Bartholomew held court on a discarded milk crate, surrounded by a gang of scruffy felines. This was clearly serious cat business.\n",
      "\n",
      "\"Well, well, well,\" Bartholomew rasped, his voice like sandpaper. \"Look what the cat dragged in. A fluffy housecat, come to admire our street cred?\"\n",
      "\n",
      "Jasper, puffed up with bravado (and a little bit of adrenaline), straightened his fur. \"I am here for adventure,\" he declared. \"Not to admire anyone's‚Ä¶ cred.\"\n",
      "\n",
      "Bartholomew chuckled, a low rumble in his chest. \"Adventure? You think chasing butterflies is adventure? We're talking real adventure, kitten. The legend of the Whispering Warehouse.\"\n",
      "\n",
      "Jasper's ears pricked. \"The Whispering Warehouse?\"\n",
      "\n",
      "Bartholomew leaned closer, his breath smelling of fish scraps and secrets. \"They say it's filled with treasures, guarded by‚Ä¶ well, that's the adventure, isn't it?\"\n",
      "\n",
      "And so, Jasper found himself embroiled in a quest. He joined Bartholomew and his ragtag crew, navigating alleyways, scaling fences, and dodging grumpy dogs. They faced challenges beyond his wildest housecat dreams ‚Äì a ferocious chihuahua named Napoleon, a leaky sewer grate, and the agonizing wait for the garbage truck to pass.\n",
      "\n",
      "Finally, they reached the Whispering Warehouse, a towering structure with boarded-up windows and an air of eerie silence. The air hummed with an unknown energy. Bartholomew, despite his bravado, looked nervous.\n",
      "\n",
      "Inside, the warehouse was a labyrinth of forgotten goods. Dust motes danced in the dim light, illuminating piles of discarded furniture and forgotten toys. But there, in the center of the room, was a pile of‚Ä¶ yarn.\n",
      "\n",
      "Mountains and mountains of yarn. In every color imaginable. Soft, fluffy, irresistible yarn.\n",
      "\n",
      "For Jasper, it was more valuable than gold. He launched himself into the pile, burying himself in the comforting texture. He purred with delight, batting and kneading, lost in a world of woolly bliss.\n",
      "\n",
      "Bartholomew and his crew stared in disbelief. \"This is it?\" Bartholomew croaked. \"All this for‚Ä¶ yarn?\"\n",
      "\n",
      "Jasper, momentarily emerging from his yarn coma, shrugged. \"Adventure is what you make it,\" he said, before disappearing back into the fluffy abyss.\n",
      "\n",
      "Later, as the sun began to set, Jasper emerged from the warehouse, a single strand of vibrant blue yarn trailing from his whiskers. He bid farewell to Bartholomew and his crew, his heart full of gratitude and a strange, fuzzy sense of accomplishment.\n",
      "\n",
      "He slipped back through the ajar back door, unnoticed. Mrs. Higgins was still preoccupied with the llama. Jasper, exhausted but exhilarated, curled up in his favorite sunbeam.\n",
      "\n",
      "The adventure had changed him. He wasn't just a fluffy housecat anymore. He was Jasper, the Explorer, the Yarn Adventurer, the‚Ä¶ Sleepy Cat. He yawned, tucked his paws under his chin, and drifted off to sleep, dreaming of mountains of yarn and the thrill of the unknown. He had a feeling this was just the beginning.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_config = types.GenerateContentConfig(\n",
    "    # Estos son los valores predeterminados para gemini-2.0-flash.\n",
    "    temperature=1.0,\n",
    "    top_p=0.95,\n",
    ")\n",
    "\n",
    "story_prompt = \"Eres un escritor creativo. Escribe una historia corta sobre un gato que se embarca en una aventura.\"\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=model_config,\n",
    "    contents=story_prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMrYs1koY6DX"
   },
   "source": [
    "## Creaci√≥n de Prompts\n",
    "\n",
    "Esta secci√≥n contiene algunos prompts del cap√≠tulo para que los pruebes directamente en la API. Intenta cambiar el texto aqu√≠ para ver c√≥mo se desempe√±a cada prompt con diferentes instrucciones, m√°s ejemplos o cualquier otro cambio que se te ocurra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhj_tQidZJP7"
   },
   "source": [
    "### Zero-shot\n",
    "\n",
    "Los prompts zero-shot son prompts que describen la solicitud para el modelo directamente.\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/1gzKKgDHwkAvexG5Up0LMtl1-6jKMKe4g\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Abrir en AI Studio</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:04:10.686115Z",
     "iopub.status.busy": "2025-03-31T10:04:10.685842Z",
     "iopub.status.idle": "2025-03-31T10:05:18.675235Z",
     "shell.execute_reply": "2025-03-31T10:05:18.674173Z",
     "shell.execute_reply.started": "2025-03-31T10:04:10.686088Z"
    },
    "id": "1_t-cwnDZzbH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIVE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_config = types.GenerateContentConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=1,\n",
    "    max_output_tokens=5,\n",
    ")\n",
    "\n",
    "zero_shot_prompt = \"\"\"Clasifica las rese√±as de pel√≠culas como POSITIVAS, NEUTRALES o NEGATIVAS.\n",
    "Rese√±a: \"Her\" es un estudio perturbador que revela la direcci√≥n\n",
    "a la que se dirige la humanidad si se permite que la IA siga evolucionando,\n",
    "sin control. Ojal√° hubiera m√°s pel√≠culas como esta obra maestra.\n",
    "Sentimiento: \"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=model_config,\n",
    "    contents=zero_shot_prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b5568bdeb11"
   },
   "source": [
    "#### Modo Enum\n",
    "\n",
    "Los modelos est√°n entrenados para generar texto, y aunque los modelos Gemini 2.0 son excelentes para seguir instrucciones, otros modelos a veces pueden producir m√°s texto del que deseas. En el ejemplo anterior, el modelo generar√° la etiqueta, pero a veces puede incluir una etiqueta \"Sentimiento\" precedente, y sin un l√≠mite de tokens de salida, tambi√©n puede agregar texto explicativo despu√©s. Consulta [este prompt en AI Studio](https://aistudio.google.com/prompts/1gzKKgDHwkAvexG5Up0LMtl1-6jKMKe4g) para ver un ejemplo.\n",
    "\n",
    "La API de Gemini tiene una funci√≥n de [modo Enum](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Enum.ipynb) que te permite restringir la salida a un conjunto fijo de valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:05:18.677294Z",
     "iopub.status.busy": "2025-03-31T10:05:18.676860Z",
     "iopub.status.idle": "2025-03-31T10:05:19.165251Z",
     "shell.execute_reply": "2025-03-31T10:05:19.164026Z",
     "shell.execute_reply.started": "2025-03-31T10:05:18.677246Z"
    },
    "id": "ad118a56c598"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "import enum\n",
    "\n",
    "class Sentiment(enum.Enum):\n",
    "    POSITIVE = \"positive\"\n",
    "    NEUTRAL = \"neutral\"\n",
    "    NEGATIVE = \"negative\"\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=types.GenerateContentConfig(\n",
    "        response_mime_type=\"text/x.enum\",\n",
    "        response_schema=Sentiment\n",
    "    ),\n",
    "    contents=zero_shot_prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al usar una salida restringida como un enum, el SDK de Python intentar√° convertir la respuesta de texto del modelo en un objeto de Python autom√°ticamente. Se almacena en el campo `response.parsed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:05:19.167261Z",
     "iopub.status.busy": "2025-03-31T10:05:19.166944Z",
     "iopub.status.idle": "2025-03-31T10:05:19.173273Z",
     "shell.execute_reply": "2025-03-31T10:05:19.171617Z",
     "shell.execute_reply.started": "2025-03-31T10:05:19.167229Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment.POSITIVE\n",
      "<enum 'Sentiment'>\n"
     ]
    }
   ],
   "source": [
    "enum_response = response.parsed\n",
    "print(enum_response)\n",
    "print(type(enum_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0udiSwNbv45W"
   },
   "source": [
    "### One-shot y few-shot\n",
    "\n",
    "Proporcionar un ejemplo de la respuesta esperada se conoce como un prompt \"one-shot\". Cuando proporcionas m√∫ltiples ejemplos, es un prompt \"few-shot\".\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/1jjWkjUSoMXmLvMJ7IzADr_GxHPJVV2bg\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Abrir en AI Studio</a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:05:19.175570Z",
     "iopub.status.busy": "2025-03-31T10:05:19.175026Z",
     "iopub.status.idle": "2025-03-31T10:05:19.707971Z",
     "shell.execute_reply": "2025-03-31T10:05:19.706895Z",
     "shell.execute_reply.started": "2025-03-31T10:05:19.175463Z"
    },
    "id": "hd4mVUukwOKZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\"size\": \"large\",\n",
      "\"type\": \"normal\",\n",
      "\"ingredients\": [\"cheese\", \"pineapple\"]\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "few_shot_prompt = \"\"\"Parsea el pedido de pizza de un cliente en JSON v√°lido:\n",
    "\n",
    "EJEMPLO:\n",
    "Quiero una pizza peque√±a con queso, salsa de tomate y pepperoni.\n",
    "Respuesta JSON:\n",
    "```\n",
    "{\n",
    "\"size\": \"small\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"cheese\", \"tomato sauce\", \"pepperoni\"]\n",
    "}\n",
    "```\n",
    "\n",
    "EJEMPLO:\n",
    "¬øPuedo obtener una pizza grande con salsa de tomate, albahaca y mozzarella?\n",
    "Respuesta JSON:\n",
    "```\n",
    "{\n",
    "\"size\": \"large\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
    "}\n",
    "```\n",
    "\n",
    "PEDIDO:\n",
    "\"\"\"\n",
    "\n",
    "customer_order = \"Dame una grande con queso y pi√±a\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=250,\n",
    "    ),\n",
    "    contents=[few_shot_prompt, customer_order])\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "021293096f08"
   },
   "source": [
    "#### Modo JSON\n",
    "\n",
    "Para proporcionar control sobre el esquema y asegurarte de que solo recibas JSON (sin otro texto o markdown), puedes usar el [modo JSON](https://github.com/google-gemini/cookbook/blob/main/quickstarts/JSON_mode.ipynb) de la API de Gemini. Esto fuerza al modelo a restringir la decodificaci√≥n, de modo que la selecci√≥n de tokens se gu√≠e por el esquema proporcionado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:05:19.709562Z",
     "iopub.status.busy": "2025-03-31T10:05:19.709202Z",
     "iopub.status.idle": "2025-03-31T10:05:20.328697Z",
     "shell.execute_reply": "2025-03-31T10:05:20.327445Z",
     "shell.execute_reply.started": "2025-03-31T10:05:19.709526Z"
    },
    "id": "50fbf0260912"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"size\": \"large\",\n",
      "  \"ingredients\": [\"apple\", \"chocolate\"],\n",
      "  \"type\": \"dessert pizza\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import typing_extensions as typing\n",
    "\n",
    "class PizzaOrder(typing.TypedDict):\n",
    "    size: str\n",
    "    ingredients: list[str]\n",
    "    type: str\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=0.1,\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=PizzaOrder,\n",
    "    ),\n",
    "    contents=\"¬øPuedo tener una pizza de postre grande con manzana y chocolate?\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4a93e338e57c"
   },
   "source": [
    "### Cadena de Pensamiento (CoT)\n",
    "\n",
    "El prompting directo en LLMs puede devolver respuestas r√°pidamente y (en t√©rminos de uso de tokens de salida) de manera eficiente, pero pueden ser propensos a la alucinaci√≥n. La respuesta puede \"parecer\" correcta (en t√©rminos de lenguaje y sintaxis) pero es incorrecta en t√©rminos de veracidad y razonamiento.\n",
    "\n",
    "El prompting de Cadena de Pensamiento es una t√©cnica en la que instruyes al modelo para que genere pasos de razonamiento intermedios, y t√≠picamente obtiene mejores resultados, especialmente cuando se combina con ejemplos few-shot. Vale la pena se√±alar que esta t√©cnica no elimina completamente las alucinaciones, y que tiende a costar m√°s ejecutarla, debido al aumento en el conteo de tokens.\n",
    "\n",
    "Modelos como la familia Gemini est√°n entrenados para ser \"conversadores\" o \"reflexivos\" y proporcionar√°n pasos de razonamiento sin prompting, por lo que para este simple ejemplo puedes pedirle al modelo que sea m√°s directo en el prompt para forzar una respuesta sin razonamiento. Intenta volver a ejecutar este paso si el modelo tiene suerte y obtiene la respuesta correcta en el primer intento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:05:20.330088Z",
     "iopub.status.busy": "2025-03-31T10:05:20.329790Z",
     "iopub.status.idle": "2025-03-31T10:05:20.741339Z",
     "shell.execute_reply": "2025-03-31T10:05:20.740335Z",
     "shell.execute_reply.started": "2025-03-31T10:05:20.330052Z"
    },
    "id": "5715555db1c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Cuando ten√≠a 4 a√±os, mi pareja ten√≠a 3 veces mi edad. Ahora, tengo 20 a√±os. ¬øCu√°ntos a√±os tiene mi pareja? Devuelve la respuesta directamente.\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e12b19677bfd"
   },
   "source": [
    "Ahora intenta el mismo enfoque, pero indica al modelo que \"piense paso a paso\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:05:20.742706Z",
     "iopub.status.busy": "2025-03-31T10:05:20.742394Z",
     "iopub.status.idle": "2025-03-31T10:05:21.530187Z",
     "shell.execute_reply": "2025-03-31T10:05:21.529098Z",
     "shell.execute_reply.started": "2025-03-31T10:05:20.742677Z"
    },
    "id": "ffd7536a481f"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's how to solve this:\n",
       "\n",
       "1. **Find the age difference:** When you were 4, your partner was 3 times your age, meaning they were 4 * 3 = 12 years old.\n",
       "\n",
       "2. **Calculate the age difference:** The age difference between you and your partner is 12 - 4 = 8 years.\n",
       "\n",
       "3. **Determine partner's current age:** Since the age difference remains constant, your partner is currently 20 + 8 = 28 years old.\n",
       "\n",
       "**Answer:** Your partner is currently 28 years old.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"Cuando ten√≠a 4 a√±os, mi pareja ten√≠a 3 veces mi edad. Ahora,\n",
    "tengo 20 a√±os. ¬øCu√°ntos a√±os tiene mi pareja? Pensemos paso a paso.\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=prompt)\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiLgBQJj0V53"
   },
   "source": [
    "### ReAct: Razonar y actuar\n",
    "\n",
    "En este ejemplo, ejecutar√°s un prompt ReAct directamente en la API de Gemini y realizar√°s los pasos de b√∫squeda t√∫ mismo. Como este prompt sigue una estructura bien definida, existen marcos disponibles que envuelven el prompt en APIs m√°s f√°ciles de usar que realizan llamadas a herramientas autom√°ticamente, como el ejemplo de LangChain del documento t√©cnico sobre \"Prompting\".\n",
    "\n",
    "Para probar esto con el motor de b√∫squeda de Wikipedia, consulta el [ejemplo de b√∫squeda en Wikipedia con ReAct](https://github.com/google-gemini/cookbook/blob/main/examples/Search_Wikipedia_using_ReAct.ipynb) en el libro de recetas.\n",
    "\n",
    "\n",
    "> Nota: El prompt y los ejemplos en contexto utilizados aqu√≠ son de [https://github.com/ysymyth/ReAct](https://github.com/ysymyth/ReAct) que se publica bajo una [licencia MIT](https://opensource.org/licenses/MIT), Copyright (c) 2023 Shunyu Yao.\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/18oo63Lwosd-bQ6Ay51uGogB3Wk3H8XMO\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Abrir en AI Studio</a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:05:21.532172Z",
     "iopub.status.busy": "2025-03-31T10:05:21.531769Z",
     "iopub.status.idle": "2025-03-31T10:05:21.539630Z",
     "shell.execute_reply": "2025-03-31T10:05:21.538374Z",
     "shell.execute_reply.started": "2025-03-31T10:05:21.532126Z"
    },
    "id": "cBgyNJ5z0VSs"
   },
   "outputs": [],
   "source": [
    "model_instructions = \"\"\"\n",
    "Resuelve una tarea de respuesta a preguntas con pasos intercalados de Pensamiento, Acci√≥n y Observaci√≥n. El Pensamiento puede razonar sobre la situaci√≥n actual,\n",
    "la Observaci√≥n es comprender informaci√≥n relevante de la salida de una Acci√≥n y la Acci√≥n puede ser de tres tipos:\n",
    " (1) <search>entidad</search>, que busca la entidad exacta en Wikipedia y devuelve el primer p√°rrafo si existe. Si no,\n",
    "     devolver√° algunas entidades similares para buscar y puedes intentar buscar la informaci√≥n de esos temas.\n",
    " (2) <lookup>palabra clave</lookup>, que devuelve la siguiente oraci√≥n que contiene la palabra clave en el contexto actual. Esto solo hace coincidencias exactas,\n",
    "     as√≠ que mant√©n tus b√∫squedas cortas.\n",
    " (3) <finish>respuesta</finish>, que devuelve la respuesta y finaliza la tarea.\n",
    "\"\"\"\n",
    "\n",
    "example1 = \"\"\"Pregunta\n",
    "El m√∫sico y satirista Allie Goertz escribi√≥ una canci√≥n sobre el personaje de \"Los Simpson\" Milhouse, ¬øa qui√©n nombr√≥ Matt Groening?\n",
    "\n",
    "Pensamiento 1\n",
    "La pregunta se simplifica a \"El personaje de Los Simpson Milhouse es nombrado despu√©s de qui√©n\". Solo necesito buscar Milhouse y encontrar a qui√©n est√° nombrado.\n",
    "\n",
    "Acci√≥n 1\n",
    "<search>Milhouse</search>\n",
    "\n",
    "Observaci√≥n 1\n",
    "Milhouse Mussolini Van Houten es un personaje recurrente en la serie de televisi√≥n animada de Fox Los Simpson, con la voz de Pamela Hayden y creado por Matt Groening.\n",
    "\n",
    "Pensamiento 2\n",
    "El p√°rrafo no dice a qui√©n est√° nombrado Milhouse, tal vez pueda buscar \"nombrado despu√©s de\".\n",
    "\n",
    "Acci√≥n 2\n",
    "<lookup>nombrado despu√©s de</lookup>\n",
    "\n",
    "Observaci√≥n 2\n",
    "Milhouse fue nombrado despu√©s del presidente de EE. UU. Richard Nixon, cuyo segundo nombre era Milhous.\n",
    "\n",
    "Pensamiento 3\n",
    "Milhouse fue nombrado despu√©s del presidente de EE. UU. Richard Nixon, as√≠ que la respuesta es Richard Nixon.\n",
    "\n",
    "Acci√≥n 3\n",
    "<finish>Richard Nixon</finish>\n",
    "\"\"\"\n",
    "\n",
    "example2 = \"\"\"Pregunta\n",
    "¬øCu√°l es el rango de elevaci√≥n para el √°rea en la que se extiende el sector oriental de la orogenia de Colorado?\n",
    "\n",
    "Pensamiento 1\n",
    "Necesito buscar la orogenia de Colorado, encontrar el √°rea en la que se extiende el sector oriental de la orogenia de Colorado, luego encontrar el rango de elevaci√≥n del √°rea.\n",
    "\n",
    "Acci√≥n 1\n",
    "<search>orogenia de Colorado</search>\n",
    "\n",
    "Observaci√≥n 1\n",
    "La orogenia de Colorado fue un episodio de construcci√≥n de monta√±as (una orogenia) en Colorado y √°reas circundantes.\n",
    "\n",
    "Pensamiento 2\n",
    "No menciona el sector oriental. As√≠ que necesito buscar el sector oriental.\n",
    "\n",
    "Acci√≥n 2\n",
    "<lookup>sector oriental</lookup>\n",
    "\n",
    "Observaci√≥n 2\n",
    "El sector oriental se extiende hacia las Grandes Llanuras y se llama la orogenia de las Llanuras Centrales.\n",
    "\n",
    "Pensamiento 3\n",
    "El sector oriental de la orogenia de Colorado se extiende hacia las Grandes Llanuras. As√≠ que necesito buscar las Grandes Llanuras y encontrar su rango de elevaci√≥n.\n",
    "\n",
    "Acci√≥n 3\n",
    "<search>Grandes Llanuras</search>\n",
    "\n",
    "Observaci√≥n 3\n",
    "Las Grandes Llanuras se refieren a una de dos regiones terrestres distintas\n",
    "\n",
    "Pensamiento 4\n",
    "Necesito buscar en su lugar las Grandes Llanuras (Estados Unidos).\n",
    "\n",
    "Acci√≥n 4\n",
    "<search>Grandes Llanuras (Estados Unidos)</search>\n",
    "\n",
    "Observaci√≥n 4\n",
    "Las Grandes Llanuras son una subregi√≥n de las Grandes Llanuras. De este a oeste, las Grandes Llanuras se elevan en elevaci√≥n desde alrededor de 1,800 a 7,000 pies (550 a 2,130m).\n",
    "\n",
    "Pensamiento 5\n",
    "Las Grandes Llanuras se elevan en elevaci√≥n desde alrededor de 1,800 a 7,000 pies, as√≠ que la respuesta es 1,800 a 7,000 pies.\n",
    "\n",
    "Acci√≥n 5\n",
    "<finish>1,800 a 7,000 pies</finish>\n",
    "\"\"\"\n",
    "\n",
    "# Inventa m√°s ejemplos t√∫ mismo, o echa un vistazo a https://github.com/ysymyth/ReAct/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3wbfstjTgey"
   },
   "source": [
    "Para capturar un solo paso a la vez, mientras ignoras cualquier paso de Observaci√≥n alucinado, usar√°s `stop_sequences` para finalizar el proceso de generaci√≥n. Los pasos son `Pensamiento`, `Acci√≥n`, `Observaci√≥n`, en ese orden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:05:21.541697Z",
     "iopub.status.busy": "2025-03-31T10:05:21.541288Z",
     "iopub.status.idle": "2025-03-31T10:05:22.176046Z",
     "shell.execute_reply": "2025-03-31T10:05:22.174821Z",
     "shell.execute_reply.started": "2025-03-31T10:05:21.541663Z"
    },
    "id": "8mxrXRkRTdXm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 1\n",
      "I need to find the transformers NLP paper and look for the author information. Then find the youngest author.\n",
      "\n",
      "Action 1\n",
      "<search>transformers NLP paper</search>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"Pregunta\n",
    "¬øQui√©n fue el autor m√°s joven listado en el art√≠culo de transformers NLP?\n",
    "\"\"\"\n",
    "\n",
    "# Realizar√°s la Acci√≥n; as√≠ que genera hasta, pero no incluyendo, la Observaci√≥n.\n",
    "react_config = types.GenerateContentConfig(\n",
    "    stop_sequences=[\"\\nObservaci√≥n\"],\n",
    "    system_instruction=model_instructions + example1 + example2,\n",
    ")\n",
    "\n",
    "# Crea un chat que tenga las instrucciones del modelo y los ejemplos pre-cargados.\n",
    "react_chat = client.chats.create(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=react_config,\n",
    ")\n",
    "\n",
    "resp = react_chat.send_message(question)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aW2PIdLbVv3l"
   },
   "source": [
    "Ahora puedes realizar esta investigaci√≥n t√∫ mismo y proporcionarla de nuevo al modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:05:22.177839Z",
     "iopub.status.busy": "2025-03-31T10:05:22.177457Z",
     "iopub.status.idle": "2025-03-31T10:05:22.733581Z",
     "shell.execute_reply": "2025-03-31T10:05:22.732393Z",
     "shell.execute_reply.started": "2025-03-31T10:05:22.177792Z"
    },
    "id": "mLMc0DZaV9g2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 2\n",
      "I have found the paper, now I need to find the age of each author and find the youngest. This is difficult without searching each author individually. I will try to search for the ages of the authors.\n",
      "\n",
      "Action 2\n",
      "<search>Ashish Vaswani age</search>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "observation = \"\"\"Observaci√≥n 1\n",
    "[1706.03762] Attention Is All You Need\n",
    "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
    "Proponemos una nueva arquitectura de red simple, el Transformer, basada √∫nicamente en mecanismos de atenci√≥n, prescindiendo completamente de la recurrencia y las convoluciones.\n",
    "\"\"\"\n",
    "resp = react_chat.send_message(observation)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bo0tzf4nX6dA"
   },
   "source": [
    "Este proceso se repite hasta que se alcanza la acci√≥n `<finish>`. Puedes continuar ejecutando esto t√∫ mismo si lo deseas, o probar el [ejemplo de Wikipedia](https://github.com/google-gemini/cookbook/blob/main/examples/Search_Wikipedia_using_ReAct.ipynb) para ver un sistema ReAct completamente automatizado en acci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modo de pensamiento\n",
    "\n",
    "El modelo experimental Gemini Flash 2.0 \"Thinking\" ha sido entrenado para generar el \"proceso de pensamiento\" que el modelo atraviesa como parte de su respuesta. Como resultado, el modelo Flash Thinking es capaz de proporcionar respuestas con capacidades de razonamiento m√°s fuertes.\n",
    "\n",
    "Usar un modelo de \"modo de pensamiento\" puede proporcionarte respuestas de alta calidad sin necesidad de prompting especializado como los enfoques anteriores. Una raz√≥n por la que esta t√©cnica es efectiva es que induces al modelo a generar informaci√≥n relevante (\"lluvia de ideas\" o \"pensamientos\") que luego se utiliza como parte del contexto en el que se genera la respuesta final.\n",
    "\n",
    "Ten en cuenta que cuando usas la API, obtienes la respuesta final del modelo, pero los pensamientos no se capturan. Para ver los pensamientos intermedios, prueba el [modelo de modo de pensamiento en AI Studio](https://aistudio.google.com/prompts/new_chat?model=gemini-2.0-flash-thinking-exp-01-21).\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/1Z991SV7lZZZqioOiqIUPv9a9ix-ws4zk\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Abrir en AI Studio</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:05:22.738995Z",
     "iopub.status.busy": "2025-03-31T10:05:22.738665Z",
     "iopub.status.idle": "2025-03-31T10:05:29.680662Z",
     "shell.execute_reply": "2025-03-31T10:05:29.679463Z",
     "shell.execute_reply.started": "2025-03-31T10:05:22.738962Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the information available online, and considering typical academic career paths, **Aidan N. Gomez** appears to be the youngest author listed on the \"Attention is All You Need\" paper, which introduced the Transformer architecture.\n",
       "\n",
       "Here's why:\n",
       "\n",
       "* **Aidan N. Gomez** was an undergraduate student at the University of Oxford at the time the paper was written.  His LinkedIn profile indicates he graduated from Oxford in 2017, the same year the paper was published.  This typically places someone in their early 20s when publishing such a paper as an undergraduate.\n",
       "\n",
       "* The other authors were researchers at Google Brain, University of Toronto, and RWTH Aachen University, and many held PhDs or were in advanced stages of their research careers. This generally suggests they were older than an undergraduate student.\n",
       "\n",
       "While precise birthdates aren't readily available for all authors to definitively confirm age,  the academic stage of Aidan N. Gomez at the time of publication strongly suggests he was the youngest author on the paper.\n",
       "\n",
       "Therefore, the most likely answer is **Aidan N. Gomez**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "from IPython.display import Markdown, clear_output\n",
    "\n",
    "\n",
    "response = client.models.generate_content_stream(\n",
    "    model='gemini-2.0-flash-thinking-exp',\n",
    "    contents='¬øQui√©n fue el autor m√°s joven listado en el art√≠culo de transformers NLP?',\n",
    ")\n",
    "\n",
    "buf = io.StringIO()\n",
    "for chunk in response:\n",
    "    buf.write(chunk.text)\n",
    "    # Muestra la respuesta a medida que se transmite\n",
    "    print(chunk.text, end='')\n",
    "\n",
    "# Y luego renderiza la respuesta final como markdown formateado.\n",
    "clear_output()\n",
    "Markdown(buf.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPiZ_eIIaVPt"
   },
   "source": [
    "## Creaci√≥n de c√≥digo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZinKamwXeR6C"
   },
   "source": [
    "### Generaci√≥n de c√≥digo\n",
    "\n",
    "La familia de modelos Gemini se puede utilizar para generar c√≥digo, configuraciones y scripts. Generar c√≥digo puede ser √∫til al aprender a programar, aprender un nuevo lenguaje o para generar r√°pidamente un primer borrador.\n",
    "\n",
    "Es importante tener en cuenta que, dado que los LLMs pueden cometer errores y pueden repetir datos de entrenamiento, es esencial leer y probar tu c√≥digo primero, y cumplir con cualquier licencia relevante.\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/1YX71JGtzDjXQkgdes8bP6i3oH5lCRKxv\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Abrir en AI Studio</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:05:29.682517Z",
     "iopub.status.busy": "2025-03-31T10:05:29.682131Z",
     "iopub.status.idle": "2025-03-31T10:05:30.209135Z",
     "shell.execute_reply": "2025-03-31T10:05:30.207926Z",
     "shell.execute_reply.started": "2025-03-31T10:05:29.682479Z"
    },
    "id": "fOQP9pqmeUO1"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def factorial(n):\n",
       "  \"\"\"Calculate the factorial of a number.\"\"\"\n",
       "  if n == 0:\n",
       "    return 1\n",
       "  else:\n",
       "    return n * factorial(n-1)\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A los modelos Gemini les encanta hablar, as√≠ que ayuda especificar que se ci√±an al c√≥digo si eso\n",
    "# es todo lo que deseas.\n",
    "code_prompt = \"\"\"\n",
    "Escribe una funci√≥n en Python para calcular el factorial de un n√∫mero. Sin explicaci√≥n, proporciona solo el c√≥digo.\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=1024,\n",
    "    ),\n",
    "    contents=code_prompt)\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlBMWSFhgVRQ"
   },
   "source": [
    "### Ejecuci√≥n de c√≥digo\n",
    "\n",
    "La API de Gemini tambi√©n puede ejecutar autom√°ticamente el c√≥digo generado y devolver√° la salida.\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/11veFr_VYEwBWcLkhNLr-maCG0G8sS_7Z\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Abrir en AI Studio</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:05:30.211033Z",
     "iopub.status.busy": "2025-03-31T10:05:30.210623Z",
     "iopub.status.idle": "2025-03-31T10:05:32.465610Z",
     "shell.execute_reply": "2025-03-31T10:05:32.464035Z",
     "shell.execute_reply.started": "2025-03-31T10:05:30.210989Z"
    },
    "id": "jT3OfWYfhjRL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Okay, I can do that. First, I will generate the first 14 odd prime '\n",
      "         'numbers. Remember that a prime number is a number greater than 1 '\n",
      "         'that has only two divisors: 1 and itself. Also, note that 2 is the '\n",
      "         'only even prime number, so all other prime numbers are odd. After '\n",
      "         'generating these numbers, I will sum them.\\n'\n",
      "         '\\n'}\n",
      "-----\n",
      "{'executable_code': {'code': 'def is_prime(n):\\n'\n",
      "                             '    if n <= 1:\\n'\n",
      "                             '        return False\\n'\n",
      "                             '    if n <= 3:\\n'\n",
      "                             '        return True\\n'\n",
      "                             '    if n % 2 == 0 or n % 3 == 0:\\n'\n",
      "                             '        return False\\n'\n",
      "                             '    i = 5\\n'\n",
      "                             '    while i * i <= n:\\n'\n",
      "                             '        if n % i == 0 or n % (i + 2) == 0:\\n'\n",
      "                             '            return False\\n'\n",
      "                             '        i += 6\\n'\n",
      "                             '    return True\\n'\n",
      "                             '\\n'\n",
      "                             'primes = []\\n'\n",
      "                             'num = 3\\n'\n",
      "                             'while len(primes) < 14:\\n'\n",
      "                             '    if is_prime(num):\\n'\n",
      "                             '        primes.append(num)\\n'\n",
      "                             '    num += 2\\n'\n",
      "                             '\\n'\n",
      "                             \"print(f'{primes=}')\\n\"\n",
      "                             '\\n'\n",
      "                             'sum_of_primes = sum(primes)\\n'\n",
      "                             \"print(f'{sum_of_primes=}')\\n\",\n",
      "                     'language': 'PYTHON'}}\n",
      "-----\n",
      "{'code_execution_result': {'outcome': 'OUTCOME_OK',\n",
      "                           'output': 'primes=[3, 5, 7, 11, 13, 17, 19, 23, 29, '\n",
      "                                     '31, 37, 41, 43, 47]\\n'\n",
      "                                     'sum_of_primes=326\\n'}}\n",
      "-----\n",
      "{'text': 'The first 14 odd prime numbers are 3, 5, 7, 11, 13, 17, 19, 23, 29, '\n",
      "         '31, 37, 41, 43, and 47.\\n'\n",
      "         '\\n'\n",
      "         'The sum of these prime numbers is 326.\\n'}\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "config = types.GenerateContentConfig(\n",
    "    tools=[types.Tool(code_execution=types.ToolCodeExecution())],\n",
    ")\n",
    "\n",
    "code_exec_prompt = \"\"\"\n",
    "Genera los primeros 14 n√∫meros primos impares, luego calcula su suma.\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=config,\n",
    "    contents=code_exec_prompt)\n",
    "\n",
    "for part in response.candidates[0].content.parts:\n",
    "  pprint(part.to_json_dict())\n",
    "  print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZspT1GSkjG6d"
   },
   "source": [
    "Esta respuesta contiene m√∫ltiples partes, incluyendo una parte de texto de apertura y cierre que representan respuestas regulares, una parte `executable_code` que representa el c√≥digo generado y una parte `code_execution_result` que representa los resultados de ejecutar el c√≥digo generado.\n",
    "\n",
    "Puedes explorarlas individualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:05:32.467952Z",
     "iopub.status.busy": "2025-03-31T10:05:32.467378Z",
     "iopub.status.idle": "2025-03-31T10:05:32.484683Z",
     "shell.execute_reply": "2025-03-31T10:05:32.483654Z",
     "shell.execute_reply.started": "2025-03-31T10:05:32.467898Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, I can do that. First, I will generate the first 14 odd prime numbers. Remember that a prime number is a number greater than 1 that has only two divisors: 1 and itself. Also, note that 2 is the only even prime number, so all other prime numbers are odd. After generating these numbers, I will sum them.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def is_prime(n):\n",
       "    if n <= 1:\n",
       "        return False\n",
       "    if n <= 3:\n",
       "        return True\n",
       "    if n % 2 == 0 or n % 3 == 0:\n",
       "        return False\n",
       "    i = 5\n",
       "    while i * i <= n:\n",
       "        if n % i == 0 or n % (i + 2) == 0:\n",
       "            return False\n",
       "        i += 6\n",
       "    return True\n",
       "\n",
       "primes = []\n",
       "num = 3\n",
       "while len(primes) < 14:\n",
       "    if is_prime(num):\n",
       "        primes.append(num)\n",
       "    num += 2\n",
       "\n",
       "print(f'{primes=}')\n",
       "\n",
       "sum_of_primes = sum(primes)\n",
       "print(f'{sum_of_primes=}')\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "primes=[3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\n",
       "sum_of_primes=326\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The first 14 odd prime numbers are 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, and 47.\n",
       "\n",
       "The sum of these prime numbers is 326.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for part in response.candidates[0].content.parts:\n",
    "    if part.text:\n",
    "        display(Markdown(part.text))\n",
    "    elif part.executable_code:\n",
    "        display(Markdown(f'```python\\n{part.executable_code.code}\\n```'))\n",
    "    elif part.code_execution_result:\n",
    "        if part.code_execution_result.outcome != 'OUTCOME_OK':\n",
    "            display(Markdown(f'## Estado {part.code_execution_result.outcome}'))\n",
    "\n",
    "        display(Markdown(f'```\\n{part.code_execution_result.output}\\n```'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gUX8QzCj4d5"
   },
   "source": [
    "### Explicaci√≥n de c√≥digo\n",
    "\n",
    "La familia de modelos Gemini tambi√©n puede explicarte el c√≥digo. En este ejemplo, pasas un [script de bash](https://github.com/magicmonty/bash-git-prompt) y haces algunas preguntas.\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/1N7LGzWzCYieyOf_7bAG4plrmkpDNmUyb\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Abrir en AI Studio</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:05:32.486266Z",
     "iopub.status.busy": "2025-03-31T10:05:32.485947Z",
     "iopub.status.idle": "2025-03-31T10:05:34.472037Z",
     "shell.execute_reply": "2025-03-31T10:05:34.471006Z",
     "shell.execute_reply.started": "2025-03-31T10:05:32.486236Z"
    },
    "id": "7_jPMMoxkIEb"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This file, `git-prompt.sh`, is a shell script designed to enhance your command-line prompt when working with Git repositories.  It provides information about the current Git status directly in your prompt, such as the branch name, the state of tracked/untracked files, and the divergence from the remote repository.\n",
       "\n",
       "**In essence, it's a cosmetic and productivity tool for developers using Git.**\n",
       "\n",
       "Here's a breakdown of why you might use it:\n",
       "\n",
       "*   **Git Status at a Glance:**  Instead of constantly running `git status`, the script displays key information (branch, modified files, etc.) directly in your prompt.\n",
       "*   **Improved Workflow:**  Knowing the Git status immediately helps you avoid mistakes and stay organized.\n",
       "*   **Customization:**  The script offers many options for customizing the prompt's appearance, including colors, symbols, and the information displayed.\n",
       "*   **Virtual Environment awareness**: The script also shows any virtual environments that you are working on, such as virtualenv, node virtualenv, and conda environments.\n",
       "\n",
       "In short, this file helps you create a more informative and visually appealing command-line prompt that integrates seamlessly with Git.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_contents = !curl https://raw.githubusercontent.com/magicmonty/bash-git-prompt/refs/heads/master/gitprompt.sh\n",
    "\n",
    "explain_prompt = f\"\"\"\n",
    "Por favor, explica qu√© hace este archivo a un nivel muy alto. ¬øQu√© es y por qu√© lo usar√≠a?\n",
    "\n",
    "```\n",
    "{file_contents}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=explain_prompt)\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a8266d97ce5"
   },
   "source": [
    "## Aprende m√°s\n",
    "\n",
    "Para aprender m√°s sobre la creaci√≥n de prompts en profundidad:\n",
    "\n",
    "* Consulta el documento t√©cnico emitido con el contenido de hoy,\n",
    "* Prueba las aplicaciones listadas en la parte superior de este notebook ([TextFX](https://textfx.withgoogle.com/), [SQL Talk](https://sql-talk-r5gdynozbq-uc.a.run.app/) y [NotebookLM](https://notebooklm.google/)),\n",
    "* Lee la [Introducci√≥n a la creaci√≥n de prompts](https://ai.google.dev/gemini-api/docs/prompting-intro) de la documentaci√≥n de la API de Gemini,\n",
    "* Explora la [galer√≠a de prompts](https://ai.google.dev/gemini-api/prompts) de la API de Gemini y pru√©balos en AI Studio,\n",
    "* Consulta el libro de recetas de la API de Gemini para [ejemplos inspiradores](https://github.com/google-gemini/cookbook/blob/main/examples/) y [inicios r√°pidos educativos](https://github.com/google-gemini/cookbook/blob/main/quickstarts/).\n",
    "\n",
    "Aseg√∫rate de revisar los laboratorios de c√≥digo del d√≠a 3 tambi√©n, donde explorar√°s una creaci√≥n de prompts m√°s avanzada con ejecuci√≥n de c√≥digo.\n",
    "\n",
    "¬°Y por favor comparte cualquier cosa emocionante que hayas probado en el Discord!\n",
    "\n",
    "*- [Mark McD](https://linktr.ee/markmcd)*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "day-1-prompting.ipynb",
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
