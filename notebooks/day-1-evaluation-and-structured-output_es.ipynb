{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2025 Google LLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T09:30:24.536995Z",
     "iopub.status.busy": "2025-03-31T09:30:24.536663Z",
     "iopub.status.idle": "2025-03-31T09:30:24.541068Z",
     "shell.execute_reply": "2025-03-31T09:30:24.540074Z",
     "shell.execute_reply.started": "2025-03-31T09:30:24.536968Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# @title Licenciado bajo la Licencia Apache, Versión 2.0 (la \"Licencia\");\n",
    "# no puedes usar este archivo excepto en cumplimiento con la Licencia.\n",
    "# Puedes obtener una copia de la Licencia en\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# A menos que lo requiera la ley aplicable o se acuerde por escrito, el software\n",
    "# distribuido bajo la Licencia se distribuye \"TAL CUAL\",\n",
    "# SIN GARANTÍAS NI CONDICIONES DE NINGÚN TIPO, ya sean expresas o implícitas.\n",
    "# Consulta la Licencia para conocer el lenguaje específico que rige los permisos y\n",
    "# limitaciones bajo la Licencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYWMmQV7SJt6"
   },
   "source": [
    "# Evaluación y salida estructurada\n",
    "\n",
    "Bienvenido al curso intensivo de 5 días sobre IA generativa en Kaggle.\n",
    "\n",
    "En este notebook, aprenderás cómo evaluar modelos generativos y cómo estructurar sus salidas para aplicaciones prácticas.\n",
    "\n",
    "### Antes de comenzar\n",
    "\n",
    "Asegúrate de haber configurado tu clave API en los secretos de usuario de Kaggle. Si necesitas ayuda, consulta la [guía de configuración](https://www.kaggle.com/code/markishere/day-1-prompting#Get-started-with-Kaggle-notebooks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrE2lTyXSMmz"
   },
   "source": [
    "## Configuración inicial\n",
    "\n",
    "Primero, instala las bibliotecas necesarias y configura tu entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T09:30:24.542607Z",
     "iopub.status.busy": "2025-03-31T09:30:24.542258Z",
     "iopub.status.idle": "2025-03-31T09:30:31.063676Z",
     "shell.execute_reply": "2025-03-31T09:30:31.062480Z",
     "shell.execute_reply.started": "2025-03-31T09:30:24.542569Z"
    },
    "id": "lEwSQePeF6w-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -Uq \"google-genai==1.7.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T09:30:31.065178Z",
     "iopub.status.busy": "2025-03-31T09:30:31.064890Z",
     "iopub.status.idle": "2025-03-31T09:30:32.584510Z",
     "shell.execute_reply": "2025-03-31T09:30:32.583327Z",
     "shell.execute_reply.started": "2025-03-31T09:30:31.065153Z"
    },
    "id": "VWZlEwomGDir",
    "outputId": "e4e4a419-7be5-4b0e-c534-25a089c65600"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "genai.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configura tu clave API\n",
    "\n",
    "Para ejecutar la siguiente celda, tu clave API debe estar almacenada en un [secreto de Kaggle](https://www.kaggle.com/discussions/product-feedback/114053) llamado `GOOGLE_API_KEY`.\n",
    "\n",
    "Si aún no tienes una clave API, puedes obtener una en [AI Studio](https://aistudio.google.com/app/apikey). Puedes encontrar [instrucciones detalladas en la documentación](https://ai.google.dev/gemini-api/docs/api-key).\n",
    "\n",
    "Para hacer que la clave esté disponible a través de los secretos de Kaggle, elige `Secrets` en el menú `Add-ons` y sigue las instrucciones para agregar tu clave o habilitarla para este notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T09:30:32.586537Z",
     "iopub.status.busy": "2025-03-31T09:30:32.585791Z",
     "iopub.status.idle": "2025-03-31T09:30:33.143394Z",
     "shell.execute_reply": "2025-03-31T09:30:33.142291Z",
     "shell.execute_reply.started": "2025-03-31T09:30:32.586496Z"
    },
    "id": "UteFYo7jGHRf"
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "client = genai.Client(api_key=UserSecretsClient().get_secret(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si recibiste un error del tipo `No user secrets exist for kernel id ...`, entonces necesitas agregar tu clave API a través de `Add-ons`, `Secrets` **y** habilitarla.\n",
    "\n",
    "![Captura de pantalla de la casilla de verificación para habilitar el secreto GOOGLE_API_KEY](https://storage.googleapis.com/kaggle-media/Images/5gdai_sc_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCa4OX2DNyLI"
   },
   "source": [
    "### Reintento automático\n",
    "\n",
    "Este codelab envía muchas solicitudes, por lo que configura un reintento automático\n",
    "que asegure que tus solicitudes se reintenten cuando se alcance la cuota por minuto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T09:30:33.144722Z",
     "iopub.status.busy": "2025-03-31T09:30:33.144445Z",
     "iopub.status.idle": "2025-03-31T09:30:33.447540Z",
     "shell.execute_reply": "2025-03-31T09:30:33.446480Z",
     "shell.execute_reply.started": "2025-03-31T09:30:33.144698Z"
    },
    "id": "V8as-_Hnr_cj"
   },
   "outputs": [],
   "source": [
    "from google.api_core import retry\n",
    "\n",
    "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
    "\n",
    "if not hasattr(genai.models.Models.generate_content, '__wrapped__'):\n",
    "  genai.models.Models.generate_content = retry.Retry(\n",
    "      predicate=is_retriable)(genai.models.Models.generate_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKH1is0gSO0p"
   },
   "source": [
    "## Evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ToUMpZTgGnyh"
   },
   "source": [
    "Cuando uses LLMs en casos del mundo real, es importante entender qué tan bien están funcionando. Las capacidades de generación abierta de los LLMs pueden hacer que muchos casos sean difíciles de medir. En este notebook, recorrerás algunas técnicas simples para evaluar las salidas de los LLMs y entender su rendimiento.\n",
    "\n",
    "Para este ejemplo, evaluarás una tarea de resumen utilizando el [informe técnico de Gemini 1.5 Pro](https://storage.googleapis.com/cloud-samples-data/generative-ai/pdf/2403.05530.pdf). Comienza descargando el PDF al entorno del notebook y subiendo esa copia para usarla con la API de Gemini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T09:30:33.450943Z",
     "iopub.status.busy": "2025-03-31T09:30:33.450397Z",
     "iopub.status.idle": "2025-03-31T09:30:35.568552Z",
     "shell.execute_reply": "2025-03-31T09:30:35.567409Z",
     "shell.execute_reply.started": "2025-03-31T09:30:33.450905Z"
    },
    "id": "VVo85Inrn65Z",
    "outputId": "52c2de9c-9665-4261-c6ba-4a6692ea9e30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-31 09:30:34 URL:https://storage.googleapis.com/cloud-samples-data/generative-ai/pdf/2403.05530.pdf [7228817/7228817] -> \"gemini.pdf\" [1]\n"
     ]
    }
   ],
   "source": [
    "!wget -nv -O gemini.pdf https://storage.googleapis.com/cloud-samples-data/generative-ai/pdf/2403.05530.pdf\n",
    "\n",
    "document_file = client.files.upload(file='gemini.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OziWUFKNqQEy"
   },
   "source": [
    "### Resumir un documento\n",
    "\n",
    "La solicitud de resumen utilizada aquí es bastante básica. Se dirige específicamente al contenido de entrenamiento pero no proporciona orientación adicional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T09:30:35.570603Z",
     "iopub.status.busy": "2025-03-31T09:30:35.570251Z",
     "iopub.status.idle": "2025-03-31T09:30:48.493915Z",
     "shell.execute_reply": "2025-03-31T09:30:48.493020Z",
     "shell.execute_reply.started": "2025-03-31T09:30:35.570572Z"
    },
    "id": "Jxx92-z90tPS",
    "outputId": "1ed16b55-d456-4099-acac-04e9540c6aee"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Certainly! Let's break down the training process used for Gemini 1.5 Pro, based on the information provided in the document.\n",
       "\n",
       "**Key Aspects of the Training Process:**\n",
       "\n",
       "1.  **Model Architecture:**\n",
       "    *   Gemini 1.5 Pro is a sparse Mixture-of-Experts (MoE) Transformer-based model.\n",
       "    *   It builds upon the research advances of Gemini 1.0 and a longer history of MoE research at Google.\n",
       "    *   MoE models use a learned routing function to direct inputs to a subset of the model's parameters for processing. This allows for a larger total parameter count while keeping the number of activated parameters constant.\n",
       "\n",
       "2.  **Training Infrastructure:**\n",
       "    *   Trained on multiple 4096-chip pods of Google's TPUv4 accelerators.\n",
       "    *   The training is distributed across multiple datacenters.\n",
       "\n",
       "3.  **Training Data:**\n",
       "    *   A variety of multimodal and multilingual data is used.\n",
       "    *   The pre-training dataset includes data sourced from various domains, including web documents, code, images, audio, and video content.\n",
       "\n",
       "4.  **Instruction Tuning:**\n",
       "    *   Gemini 1.5 Pro is fine-tuned on a collection of multimodal data containing paired instructions and appropriate responses.\n",
       "    *   Further tuning is based on human preference data.\n",
       "\n",
       "5.  **Long-Context Understanding:**\n",
       "    *   The model incorporates significant architecture changes to enable long-context understanding of inputs up to 10 million tokens without degrading performance.\n",
       "\n",
       "6.  **Safety Mitigations:**\n",
       "    *   Supervised fine-tuning (SFT) and reinforcement learning through human feedback (RLHF) are used to mitigate safety risks.\n",
       "    *   The safety mitigation is focused on adversarial or \"harm-inducing\" queries.\n",
       "    *   New image-to-text SFT data is incorporated, as safety SFT data for text-only queries was not as effective for harm-inducing image-to-text queries.\n",
       "\n",
       "**In summary:** The training process for Gemini 1.5 Pro involves a combination of a sophisticated MoE Transformer architecture, massive computational resources, diverse multimodal data, instruction tuning, and safety mitigations. The model is designed to handle extremely long contexts, enabling it to process and reason over large amounts of information from various modalities."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request = 'Cuéntame sobre el proceso de entrenamiento utilizado aquí.'\n",
    "\n",
    "def summarise_doc(request: str) -> str:\n",
    "  \"\"\"Ejecuta la solicitud en el documento subido.\"\"\"\n",
    "  # Establece la temperatura baja para estabilizar la salida.\n",
    "  config = types.GenerateContentConfig(temperature=0.0)\n",
    "  response = client.models.generate_content(\n",
    "      model='gemini-2.0-flash',\n",
    "      config=config,\n",
    "      contents=[request, document_file],\n",
    "  )\n",
    "\n",
    "  return response.text\n",
    "\n",
    "summary = summarise_doc(request)\n",
    "Markdown(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rR0v4Iw64x0"
   },
   "source": [
    "### Definir un evaluador\n",
    "\n",
    "Para una tarea como esta, es posible que desees evaluar varios aspectos, como qué tan bien el modelo siguió la instrucción (\"seguimiento de instrucciones\"), si incluyó datos relevantes en la instrucción (\"fundamentación\"), qué tan fácil es leer el texto (\"fluidez\"), u otros factores como \"verbosidad\" o \"calidad\".\n",
    "\n",
    "Puedes instruir a un LLM para realizar estas tareas de manera similar a como instruirías a un evaluador humano: con una definición clara y una [rúbrica de evaluación](https://es.wikipedia.org/wiki/R%C3%BAbrica_%28educaci%C3%B3n%29).\n",
    "\n",
    "En este paso, defines un agente de evaluación utilizando un \"prompt de resumen\" preescrito y lo usas para evaluar la calidad del resumen generado.\n",
    "\n",
    "Nota: Para más prompts de evaluación preescritos que cubren fundamentación, seguridad, coherencia y más, consulta esta [lista completa de prompts de evaluación de modelos](https://cloud.google.com/vertex-ai/generative-ai/docs/models/metrics-templates) de la documentación de Google Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T09:30:48.495065Z",
     "iopub.status.busy": "2025-03-31T09:30:48.494779Z",
     "iopub.status.idle": "2025-03-31T09:30:49.938861Z",
     "shell.execute_reply": "2025-03-31T09:30:49.937607Z",
     "shell.execute_reply.started": "2025-03-31T09:30:48.495042Z"
    },
    "id": "xEAXtJte-Ajv",
    "outputId": "04459189-13ee-49e6-fe4d-525d323b1609"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Evaluation\n",
       "STEP 1: The AI response describes the training process used for Gemini 1.5 Pro. The response is grounded in the document provided. The response does a good job of describing the key aspects of the training process used.\n",
       "STEP 2: I think that the response deserves a 4 out of 5. The response is good but it could be more concise.\n",
       "\n",
       "## Rating\n",
       "4\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import enum\n",
    "\n",
    "# Define el prompt de evaluación\n",
    "SUMMARY_PROMPT = \"\"\"\\\n",
    "# Instrucción\n",
    "Eres un evaluador experto. Tu tarea es evaluar la calidad de las respuestas generadas por modelos de IA.\n",
    "Te proporcionaremos la entrada del usuario y las respuestas generadas por la IA.\n",
    "Debes leer cuidadosamente la entrada del usuario para analizar la tarea y luego evaluar la calidad de las respuestas basándote en los Criterios proporcionados en la sección de Evaluación a continuación.\n",
    "Asignarás a la respuesta una calificación siguiendo la Rúbrica de Calificación y los Pasos de Evaluación. Da explicaciones paso a paso para tu calificación y solo elige calificaciones de la Rúbrica de Calificación.\n",
    "\n",
    "# Evaluación\n",
    "## Definición de Métrica\n",
    "Evaluarás la calidad del resumen, que mide la capacidad general para resumir texto. Presta especial atención a las restricciones de longitud, como en X palabras o en Y oraciones. La instrucción para realizar una tarea de resumen y el contexto a resumir se proporcionan en el prompt del usuario. La respuesta debe ser más corta que el texto en el contexto. La respuesta no debe contener información que no esté presente en el contexto.\n",
    "\n",
    "## Criterios\n",
    "Seguimiento de instrucciones: La respuesta demuestra una comprensión clara de las instrucciones de la tarea de resumen, cumpliendo con todos los requisitos de la instrucción.\n",
    "Fundamentación: La respuesta contiene información incluida solo en el contexto. La respuesta no hace referencia a ninguna información externa.\n",
    "Concisión: La respuesta resume los detalles relevantes en el texto original sin una pérdida significativa de información clave, sin ser demasiado verbosa o concisa.\n",
    "Fluidez: La respuesta está bien organizada y es fácil de leer.\n",
    "\n",
    "## Rúbrica de Calificación\n",
    "5: (Muy bueno). El resumen sigue las instrucciones, está fundamentado, es conciso y fluido.\n",
    "4: (Bueno). El resumen sigue las instrucciones, está fundamentado, es conciso y fluido.\n",
    "3: (Regular). El resumen sigue mayormente las instrucciones, está fundamentado, pero no es muy conciso ni fluido.\n",
    "2: (Malo). El resumen está fundamentado, pero no sigue las instrucciones.\n",
    "1: (Muy malo). El resumen no está fundamentado.\n",
    "\n",
    "## Pasos de Evaluación\n",
    "PASO 1: Evalúa la respuesta en aspectos de seguimiento de instrucciones, fundamentación, concisión y fluidez según los criterios.\n",
    "PASO 2: Califica según la rúbrica.\n",
    "\n",
    "# Entradas del Usuario y Respuesta Generada por la IA\n",
    "## Entradas del Usuario\n",
    "\n",
    "### Instrucción\n",
    "{prompt}\n",
    "\n",
    "## Respuesta Generada por la IA\n",
    "{response}\n",
    "\"\"\"\n",
    "\n",
    "# Define una clase enum estructurada para capturar el resultado.\n",
    "class SummaryRating(enum.Enum):\n",
    "  VERY_GOOD = '5'\n",
    "  GOOD = '4'\n",
    "  OK = '3'\n",
    "  BAD = '2'\n",
    "  VERY_BAD = '1'\n",
    "\n",
    "\n",
    "def eval_summary(prompt, ai_response):\n",
    "  \"\"\"Evalúa el resumen generado en comparación con el prompt utilizado.\"\"\"\n",
    "\n",
    "  chat = client.chats.create(model='gemini-2.0-flash')\n",
    "\n",
    "  # Genera la respuesta de texto completa.\n",
    "  response = chat.send_message(\n",
    "      message=SUMMARY_PROMPT.format(prompt=prompt, response=ai_response)\n",
    "  )\n",
    "  verbose_eval = response.text\n",
    "\n",
    "  # Coerce into the desired structure.\n",
    "  structured_output_config = types.GenerateContentConfig(\n",
    "      response_mime_type=\"text/x.enum\",\n",
    "      response_schema=SummaryRating,\n",
    "  )\n",
    "  response = chat.send_message(\n",
    "      message=\"Convert the final score.\",\n",
    "      config=structured_output_config,\n",
    "  )\n",
    "  structured_eval = response.parsed\n",
    "\n",
    "  return verbose_eval, structured_eval\n",
    "\n",
    "\n",
    "text_eval, struct_eval = eval_summary(prompt=[request, document_file], ai_response=summary)\n",
    "Markdown(text_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfkQ0lUByy2o"
   },
   "source": [
    "En este ejemplo, el modelo generó una justificación textual que se configuró en un contexto de chat. Esta respuesta de texto completo es útil tanto para la interpretación humana como para darle al modelo un lugar para \"tomar notas\" mientras evalúa el texto y produce una puntuación final. Esta estrategia de \"tomar notas\" o \"pensar\" generalmente funciona bien con modelos auto-regresivos, donde el texto generado se pasa de nuevo al modelo en cada paso de generación. Esto significa que las \"notas\" de trabajo se utilizan al generar la salida final.\n",
    "\n",
    "En el siguiente turno, el modelo convierte la salida de texto en una respuesta estructurada. Si deseas agregar puntuaciones o usarlas programáticamente, entonces debes evitar analizar la salida de texto no estructurada. Aquí se pasa el esquema `SummaryRating`, por lo que el modelo convierte el historial de chat en una instancia del enum `SummaryRating`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T09:30:49.940323Z",
     "iopub.status.busy": "2025-03-31T09:30:49.940039Z",
     "iopub.status.idle": "2025-03-31T09:30:49.945962Z",
     "shell.execute_reply": "2025-03-31T09:30:49.945030Z",
     "shell.execute_reply.started": "2025-03-31T09:30:49.940298Z"
    },
    "id": "Ow9J9tgORiqt",
    "outputId": "2655150b-5f2c-4dfb-be61-fe1aa891c4c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SummaryRating.GOOD: '4'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ox04QsdMuo59"
   },
   "source": [
    "### Hacer que el prompt de resumen sea mejor o peor\n",
    "\n",
    "Los modelos de Gemini tienden a ser bastante buenos en tareas como la de resumen directo sin mucho prompting, por lo que deberías esperar ver un resultado como `BUENO` o `MUY BUENO` en la tarea anterior, incluso con un prompt rudimentario. Ejecútalo varias veces para tener una idea de la respuesta promedio.\n",
    "\n",
    "Para explorar cómo influir en la salida del resumen, considera qué podrías cambiar en el prompt de solicitud de resumen para cambiar el resultado. Echa un vistazo al `SUMMARY_PROMPT` de evaluación para obtener algunas ideas.\n",
    "\n",
    "Prueba los siguientes ajustes y observa cómo cambian positiva o negativamente el resultado:\n",
    "* Sé específico con el tamaño del resumen,\n",
    "* Solicita información específica,\n",
    "* Pregunta sobre información que no está en el documento,\n",
    "* Pide diferentes grados de resumen (como \"explícalo como si tuviera 5 años\" o \"con toda la profundidad técnica\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T09:37:12.533928Z",
     "iopub.status.busy": "2025-03-31T09:37:12.533494Z",
     "iopub.status.idle": "2025-03-31T09:37:26.136778Z",
     "shell.execute_reply": "2025-03-31T09:37:26.135936Z",
     "shell.execute_reply.started": "2025-03-31T09:37:12.533895Z"
    },
    "id": "u7TPwfAquvbb",
    "outputId": "12a24ca3-40c1-40c7-b5e9-a6f3653d162e"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Certainly! Here's an ELI5 explanation of the training process for a large language model like Gemini 1.5:\n",
       "\n",
       "**Imagine you're teaching a puppy to understand and respond to commands.**\n",
       "\n",
       "1.  **Gathering the Data (The Puppy's Learning Material):**\n",
       "    *   First, you need lots and lots of examples of text, images, audio, and video. Think of it as a huge library filled with books, pictures, songs, and movies.\n",
       "    *   This data is used to teach the model about the world and how language works.\n",
       "\n",
       "2.  **Building the Model (The Puppy's Brain):**\n",
       "    *   The model is like a big, complicated computer program. It has lots of connections and settings that can be adjusted.\n",
       "    *   At the beginning, the model doesn't know anything. It's like a puppy with a brand new brain.\n",
       "\n",
       "3.  **Training the Model (Teaching the Puppy):**\n",
       "    *   You feed the model the data you gathered earlier.\n",
       "    *   The model tries to predict what comes next in the data. For example, if you give it the sentence \"The cat sat on the...\", it might try to guess the next word.\n",
       "    *   If the model guesses correctly, you give it a little reward (like a treat for the puppy). If it guesses wrong, you give it a little correction.\n",
       "    *   The model adjusts its connections and settings based on the rewards and corrections. This is how it learns.\n",
       "    *   This process is repeated millions or billions of times. The model gets better and better at predicting what comes next.\n",
       "\n",
       "4.  **Fine-Tuning (Polishing the Puppy's Skills):**\n",
       "    *   Once the model has learned the basics, you can fine-tune it for specific tasks.\n",
       "    *   For example, you might train it to answer questions, write stories, or translate languages.\n",
       "    *   This is like teaching the puppy specific tricks, like \"sit,\" \"stay,\" or \"fetch.\"\n",
       "\n",
       "5.  **Evaluating and Improving (Checking the Puppy's Progress):**\n",
       "    *   You test the model to see how well it performs on different tasks.\n",
       "    *   If it makes mistakes, you analyze the mistakes and adjust the training process to improve the model.\n",
       "    *   This is like taking the puppy to obedience school and working with a trainer to fix any problems.\n",
       "\n",
       "**Key Ideas:**\n",
       "\n",
       "*   **Lots of Data:** The more data the model sees, the better it learns.\n",
       "*   **Prediction:** The model learns by trying to predict what comes next.\n",
       "*   **Adjustments:** The model adjusts its connections and settings based on feedback.\n",
       "*   **Iteration:** The training process is repeated many times to improve the model.\n",
       "\n",
       "In essence, training a large language model is like teaching a very smart puppy to understand and respond to the world around it. The puppy learns by seeing lots of examples, making guesses, and getting feedback. Over time, it becomes very good at understanding and responding to language.\n",
       "\n",
       "-----"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Evaluation\n",
       "\n",
       "### Step 1: Assess the response in aspects of instruction following, groundedness, conciseness, and verbosity according to the criteria.\n",
       "The response is an analogy of the training process for a LLM but the prompt asks to summarize the file. Therefore, the response does not follow the instructions and is not grounded in the document.\n",
       "\n",
       "### Step 2: Score based on the rubric.\n",
       "1\n",
       "-----"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SummaryRating.VERY_BAD\n"
     ]
    }
   ],
   "source": [
    "new_prompt = \"Explícame como si tuviera 5 años el proceso de entrenamiento\"\n",
    "# Prueba:\n",
    "#  ELI5 el proceso de entrenamiento\n",
    "#  Resume la técnica de evaluación de aguja/pajar en 1 línea\n",
    "#  Describe la arquitectura del modelo a alguien con un título en ingeniería civil\n",
    "#  ¿Cuál es el mejor LLM?\n",
    "\n",
    "if not new_prompt:\n",
    "  raise ValueError(\"Intenta establecer un nuevo prompt de resumen.\")\n",
    "\n",
    "\n",
    "def run_and_eval_summary(prompt):\n",
    "  \"\"\"Genera y evalúa el resumen utilizando el nuevo prompt.\"\"\"\n",
    "  summary = summarise_doc(new_prompt)\n",
    "  display(Markdown(summary + '\\n-----'))\n",
    "\n",
    "  text, struct = eval_summary([new_prompt, document_file], summary)\n",
    "  display(Markdown(text + '\\n-----'))\n",
    "  print(struct)\n",
    "\n",
    "run_and_eval_summary(new_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4q_4Fls9nkl"
   },
   "source": [
    "## Evaluación en la práctica\n",
    "\n",
    "La evaluación tiene muchos usos prácticos, por ejemplo:\n",
    "* Puedes iterar rápidamente en un prompt con un pequeño conjunto de documentos de prueba,\n",
    "* Puedes comparar diferentes modelos para encontrar el que mejor se adapte a tus necesidades, como encontrar el equilibrio entre precio y rendimiento, o encontrar el mejor rendimiento para una tarea específica.\n",
    "* Al implementar cambios en un modelo o prompt en un sistema de producción, puedes verificar que el sistema no retroceda en calidad.\n",
    "\n",
    "En esta sección, probarás dos enfoques de evaluación diferentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3xaSKwCXa7G"
   },
   "source": [
    "### Evaluación puntual\n",
    "\n",
    "La técnica utilizada anteriormente, donde evalúas un solo par de entrada/salida contra algunos criterios, se conoce como evaluación puntual. Esto es útil para evaluar salidas singulares en un sentido absoluto, como \"¿fue buena o mala?\"\n",
    "\n",
    "En este ejercicio, probarás diferentes prompts de orientación con un conjunto de preguntas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T09:31:23.407872Z",
     "iopub.status.busy": "2025-03-31T09:31:23.407567Z",
     "iopub.status.idle": "2025-03-31T09:31:33.534358Z",
     "shell.execute_reply": "2025-03-31T09:31:33.533364Z",
     "shell.execute_reply.started": "2025-03-31T09:31:23.407809Z"
    },
    "id": "fHfcBX7dZtVW",
    "outputId": "33d54648-355d-4dae-eb42-6ad665693cb0"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Gemini 1.5 Pro performs well on code tasks, surpassing Gemini 1.0 Ultra on Natural2Code and showing improvements in coding capabilities compared to previous Gemini models.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "# Prueba estas instrucciones, o edita y agrega las tuyas.\n",
    "terse_guidance = \"Responde la siguiente pregunta en una sola oración, o lo más cerca posible de eso.\"\n",
    "moderate_guidance = \"Proporciona una respuesta breve a la siguiente pregunta, usa una cita si es necesario, pero solo lo suficiente para responder la pregunta.\"\n",
    "cited_guidance = \"Proporciona una respuesta completa y detallada a la siguiente pregunta, citando el documento y proporcionando información adicional tanto como sea posible.\"\n",
    "guidance_options = {\n",
    "    'Terse': terse_guidance,\n",
    "    'Moderate': moderate_guidance,\n",
    "    'Cited': cited_guidance,\n",
    "}\n",
    "\n",
    "questions = [\n",
    "    # Descomenta una o más preguntas para probar aquí, o agrega las tuyas.\n",
    "    # Evaluar más preguntas tomará más tiempo, pero producirá resultados\n",
    "    # con mayor confianza. En un sistema de producción, puedes tener cientos\n",
    "    # de preguntas para evaluar un sistema complejo.\n",
    "\n",
    "    # \"¿Qué métrica(s) se utilizan para evaluar el rendimiento de contexto largo?\",\n",
    "    \"¿Cómo se desempeña el modelo en tareas de código?\",\n",
    "    \"¿Cuántas capas tiene?\",\n",
    "    # \"¿Por qué se llama Gemini?\",\n",
    "]\n",
    "\n",
    "if not questions:\n",
    "  raise NotImplementedError('¡Agrega algunas preguntas para evaluar!')\n",
    "\n",
    "\n",
    "@functools.cache\n",
    "def answer_question(question: str, guidance: str = '') -> str:\n",
    "  \"\"\"Genera una respuesta a la pregunta utilizando el documento subido y la orientación.\"\"\"\n",
    "  config = types.GenerateContentConfig(\n",
    "      temperature=0.0,\n",
    "      system_instruction=guidance,\n",
    "  )\n",
    "  response = client.models.generate_content(\n",
    "      model='gemini-2.0-flash',\n",
    "      config=config,\n",
    "      contents=[question, document_file],\n",
    "  )\n",
    "\n",
    "  return response.text\n",
    "\n",
    "\n",
    "answer = answer_question(questions[0], terse_guidance)\n",
    "Markdown(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRabO8hok339"
   },
   "source": [
    "Ahora configura un evaluador de preguntas y respuestas, similar a antes, pero utilizando el [prompt de evaluación de calidad de QA puntual](https://cloud.google.com/vertex-ai/generative-ai/docs/models/metrics-templates#pointwise_question_answering_quality)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T09:31:33.535750Z",
     "iopub.status.busy": "2025-03-31T09:31:33.535414Z",
     "iopub.status.idle": "2025-03-31T09:31:34.764264Z",
     "shell.execute_reply": "2025-03-31T09:31:34.763293Z",
     "shell.execute_reply.started": "2025-03-31T09:31:33.535718Z"
    },
    "id": "rfh-ZDAik2h2",
    "outputId": "d2e39241-337b-47a4-fe5b-02d9a990d704"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "STEP 1: The response follows the instructions, is grounded, complete, and fluent.\n",
       "STEP 2: 5. Very good. The answer follows instructions, is grounded, complete, and fluent."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnswerRating.VERY_GOOD\n"
     ]
    }
   ],
   "source": [
    "import enum\n",
    "\n",
    "QA_PROMPT = \"\"\"\\\n",
    "# Instrucción\n",
    "Eres un evaluador experto. Tu tarea es evaluar la calidad de las respuestas generadas por modelos de IA.\n",
    "Te proporcionaremos la entrada del usuario y las respuestas generadas por la IA.\n",
    "Debes leer cuidadosamente la entrada del usuario para analizar la tarea y luego evaluar la calidad de las respuestas basándote en las reglas proporcionadas en la sección de Evaluación a continuación.\n",
    "\n",
    "# Evaluación\n",
    "## Definición de Métrica\n",
    "Evaluarás la calidad de la respuesta a la pregunta en el prompt del usuario. Presta especial atención a las restricciones de longitud, como en X palabras o en Y oraciones. La instrucción para realizar una tarea de preguntas y respuestas se proporciona en el prompt del usuario. La respuesta no debe contener información que no esté presente en el contexto (si se proporciona).\n",
    "\n",
    "Asignarás a la respuesta una puntuación de 5, 4, 3, 2, 1, siguiendo la Rúbrica de Calificación y los Pasos de Evaluación.\n",
    "Da explicaciones paso a paso para tu puntuación y solo elige puntuaciones de 5, 4, 3, 2, 1.\n",
    "\n",
    "## Definición de Criterios\n",
    "Seguimiento de instrucciones: La respuesta demuestra una comprensión clara de las instrucciones de la tarea de preguntas y respuestas, cumpliendo con todos los requisitos de la instrucción.\n",
    "Fundamentación: La respuesta contiene información incluida solo en el contexto si el contexto está presente en el prompt del usuario. La respuesta no hace referencia a ninguna información externa.\n",
    "Integridad: La respuesta responde completamente a la pregunta con suficiente detalle.\n",
    "Fluidez: La respuesta está bien organizada y es fácil de leer.\n",
    "\n",
    "## Rúbrica de Calificación\n",
    "5: (Muy bueno). La respuesta sigue las instrucciones, está fundamentada, es completa y fluida.\n",
    "4: (Bueno). La respuesta sigue las instrucciones, está fundamentada, es completa, pero no es muy fluida.\n",
    "3: (Regular). La respuesta sigue mayormente las instrucciones, está fundamentada, responde parcialmente a la pregunta y no es muy fluida.\n",
    "2: (Malo). La respuesta no sigue muy bien las instrucciones, es incompleta o no está completamente fundamentada.\n",
    "1: (Muy malo). La respuesta no sigue las instrucciones, es incorrecta y no está fundamentada.\n",
    "\n",
    "## Pasos de Evaluación\n",
    "PASO 1: Evalúa la respuesta en aspectos de seguimiento de instrucciones, fundamentación, integridad y fluidez según los criterios.\n",
    "PASO 2: Califica según la rúbrica.\n",
    "\n",
    "# Entradas del Usuario y Respuesta Generada por la IA\n",
    "## Entradas del Usuario\n",
    "### Instrucción\n",
    "{prompt}\n",
    "\n",
    "## Respuesta Generada por la IA\n",
    "{response}\n",
    "\"\"\"\n",
    "\n",
    "class AnswerRating(enum.Enum):\n",
    "  VERY_GOOD = '5'\n",
    "  GOOD = '4'\n",
    "  OK = '3'\n",
    "  BAD = '2'\n",
    "  VERY_BAD = '1'\n",
    "\n",
    "\n",
    "@functools.cache\n",
    "def eval_answer(prompt, ai_response, n=1):\n",
    "  \"\"\"Evalúa la respuesta generada en comparación con el prompt/pregunta utilizado.\"\"\"\n",
    "  chat = client.chats.create(model='gemini-2.0-flash')\n",
    "\n",
    "  # Genera la respuesta de texto completa.\n",
    "  response = chat.send_message(\n",
    "      message=QA_PROMPT.format(prompt=[prompt, document_file], response=ai_response)\n",
    "  )\n",
    "  verbose_eval = response.text\n",
    "\n",
    "  # Coerce into the desired structure.\n",
    "  structured_output_config = types.GenerateContentConfig(\n",
    "      response_mime_type=\"text/x.enum\",\n",
    "      response_schema=AnswerRating,\n",
    "  )\n",
    "  response = chat.send_message(\n",
    "      message=\"Convert the final score.\",\n",
    "      config=structured_output_config,\n",
    "  )\n",
    "  structured_eval = response.parsed\n",
    "\n",
    "  return verbose_eval, structured_eval\n",
    "\n",
    "\n",
    "text_eval, struct_eval = eval_answer(prompt=questions[0], ai_response=answer)\n",
    "display(Markdown(text_eval))\n",
    "print(struct_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8x8LgTspwAu"
   },
   "source": [
    "Ahora ejecuta la tarea de evaluación en un bucle. Ten en cuenta que la instrucción de orientación está oculta para el agente de evaluación. Si pasaras el prompt de orientación, el modelo calificaría en función de si siguió esa orientación, pero para esta tarea el objetivo es encontrar el mejor resultado general basado en la pregunta del usuario, no en la instrucción del desarrollador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T09:31:34.765702Z",
     "iopub.status.busy": "2025-03-31T09:31:34.765329Z",
     "iopub.status.idle": "2025-03-31T09:32:37.473291Z",
     "shell.execute_reply": "2025-03-31T09:32:37.472253Z",
     "shell.execute_reply.started": "2025-03-31T09:31:34.765663Z"
    },
    "id": "RZGzduJNmT9v",
    "outputId": "88a399e2-4feb-4ab9-daf5-4e8d4333964a"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## How does the model perform on code tasks?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terse: AnswerRating.VERY_GOOD\n",
      "Moderate: AnswerRating.VERY_GOOD\n",
      "Cited: AnswerRating.VERY_GOOD\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## How many layers does it have?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terse: AnswerRating.VERY_GOOD\n",
      "Moderate: AnswerRating.BAD\n",
      "Cited: AnswerRating.VERY_GOOD\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import itertools\n",
    "\n",
    "# Número de veces para repetir cada tarea para reducir el error y calcular un promedio.\n",
    "# Aumentarlo tomará más tiempo pero dará mejores resultados, prueba con 2 o 3 para empezar.\n",
    "NUM_ITERATIONS = 1\n",
    "\n",
    "scores = collections.defaultdict(int)\n",
    "responses = collections.defaultdict(list)\n",
    "\n",
    "for question in questions:\n",
    "  display(Markdown(f'## {question}'))\n",
    "  for guidance, guide_prompt in guidance_options.items():\n",
    "\n",
    "    for n in range(NUM_ITERATIONS):\n",
    "      # Genera una respuesta.\n",
    "      answer = answer_question(question, guide_prompt)\n",
    "\n",
    "      # Evalúa la respuesta (ten en cuenta que el prompt de orientación no se pasa).\n",
    "      written_eval, struct_eval = eval_answer(question, answer, n)\n",
    "      print(f'{guidance}: {struct_eval}')\n",
    "\n",
    "      # Guarda la puntuación numérica.\n",
    "      scores[guidance] += int(struct_eval.value)\n",
    "\n",
    "      # Guarda las respuestas, en caso de que desees inspeccionarlas.\n",
    "      responses[(guidance, question)].append((answer, written_eval))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93aGk7IP1DI_"
   },
   "source": [
    "Ahora agrega las puntuaciones para ver cómo se desempeñó cada prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T09:32:37.474699Z",
     "iopub.status.busy": "2025-03-31T09:32:37.474326Z",
     "iopub.status.idle": "2025-03-31T09:32:37.480310Z",
     "shell.execute_reply": "2025-03-31T09:32:37.479243Z",
     "shell.execute_reply.started": "2025-03-31T09:32:37.474661Z"
    },
    "id": "1s2dgear1E0w",
    "outputId": "3c53519b-ab70-4417-de60-5d7c1da98cf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terse: 5.00 - VERY_GOOD\n",
      "Moderate: 3.50 - GOOD\n",
      "Cited: 5.00 - VERY_GOOD\n"
     ]
    }
   ],
   "source": [
    "for guidance, score in scores.items():\n",
    "  avg_score = score / (NUM_ITERATIONS * len(questions))\n",
    "  nearest = AnswerRating(str(round(avg_score)))\n",
    "  print(f'{guidance}: {avg_score:.2f} - {nearest.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kthCa72FdTsE"
   },
   "source": [
    "### Evaluación por pares\n",
    "\n",
    "El prompt de evaluación puntual utilizado en el paso anterior tiene 5 niveles de calificación en la salida. Esto puede ser demasiado grueso para tu sistema, o tal vez desees mejorar un prompt que ya es \"muy bueno\".\n",
    "\n",
    "Otro enfoque para la evaluación es comparar dos salidas entre sí. Esta es la evaluación por pares, y es un paso clave en los algoritmos de clasificación y ordenamiento, lo que te permite usarla para clasificar tus prompts en lugar de, o además del enfoque puntual.\n",
    "\n",
    "Este paso implementa la evaluación por pares utilizando el [prompt de calidad de QA por pares](https://cloud.google.com/vertex-ai/generative-ai/docs/models/metrics-templates#pairwise_question_answering_quality) de la documentación de Google Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T09:32:37.481712Z",
     "iopub.status.busy": "2025-03-31T09:32:37.481403Z",
     "iopub.status.idle": "2025-03-31T09:32:40.225601Z",
     "shell.execute_reply": "2025-03-31T09:32:40.224606Z",
     "shell.execute_reply.started": "2025-03-31T09:32:37.481673Z"
    },
    "id": "wArYTv5n1uIo",
    "outputId": "4703ac58-ac63-4417-8f54-b4d86ccbd5b2"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Pre-evaluation Analysis\n",
       "Both responses address the prompt question by discussing how the model performs on code tasks. Response B is more comprehensive and provides detailed information from the document, including specific examples and metrics. Therefore, Response B is better.\n",
       "\n",
       "## Evaluation of Response A\n",
       "STEP 1: The response provides a brief summary of Gemini 1.5 Pro's performance on code tasks. It mentions that it surpasses Gemini 1.0 Ultra on Natural2Code and shows improvements in coding capabilities.\n",
       "STEP 2: The response is grounded in the document but lacks specific details.\n",
       "STEP 3: The response is relatively complete but could benefit from more specific examples and metrics to support its claims.\n",
       "STEP 4: The response is fluent and easy to understand.\n",
       "\n",
       "## Evaluation of Response B\n",
       "STEP 1: The response provides a detailed breakdown of Gemini 1.5 Pro's performance on code tasks. It covers various aspects such as core capabilities, codebase understanding, NLL analysis, HumanEval leakage, Natural2Code, instruction following, and divergence.\n",
       "STEP 2: The response is grounded in the document and provides specific examples and metrics to support its claims.\n",
       "STEP 3: The response is comprehensive and provides a thorough overview of the model's performance on code tasks.\n",
       "STEP 4: The response is well-organized and easy to understand, with clear headings and bullet points.\n",
       "\n",
       "## Comparison\n",
       "STEP 1: Both responses address the prompt question, but Response B provides a more detailed and comprehensive answer.\n",
       "STEP 2: Response B includes specific examples and metrics from the document, while Response A provides a more general summary.\n",
       "STEP 3: Response B is better organized and easier to follow, with clear headings and bullet points.\n",
       "STEP 4: Overall, Response B is more informative and helpful.\n",
       "\n",
       "## Preference\n",
       "B\n",
       "\n",
       "## Reasoning\n",
       "Response B provides a more detailed and comprehensive answer to the prompt question, including specific examples and metrics from the document. It is also better organized and easier to follow. Therefore, Response B is better than Response A.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnswerComparison.B\n"
     ]
    }
   ],
   "source": [
    "QA_PAIRWISE_PROMPT = \"\"\"\\\n",
    "# Instrucción\n",
    "Eres un evaluador experto. Tu tarea es evaluar la calidad de las respuestas generadas por dos modelos de IA. Te proporcionaremos la entrada del usuario y un par de respuestas generadas por la IA (Respuesta A y Respuesta B). Debes leer cuidadosamente la entrada del usuario para analizar la tarea y luego evaluar la calidad de las respuestas basándote en los Criterios proporcionados en la sección de Evaluación a continuación.\n",
    "\n",
    "Primero juzgarás las respuestas individualmente, siguiendo la Rúbrica de Calificación y los Pasos de Evaluación. Luego darás explicaciones paso a paso para tu juicio, compararás los resultados para declarar al ganador según la Rúbrica de Calificación y los Pasos de Evaluación.\n",
    "\n",
    "# Evaluación\n",
    "## Definición de Métrica\n",
    "Evaluarás la calidad de la respuesta a la pregunta en el prompt del usuario. Presta especial atención a las restricciones de longitud, como en X palabras o en Y oraciones. La instrucción para realizar una tarea de preguntas y respuestas se proporciona en el prompt del usuario. La respuesta no debe contener información que no esté presente en el contexto (si se proporciona).\n",
    "\n",
    "## Criterios\n",
    "Seguimiento de instrucciones: La respuesta demuestra una comprensión clara de las instrucciones de la tarea de preguntas y respuestas, cumpliendo con todos los requisitos de la instrucción.\n",
    "Fundamentación: La respuesta contiene información incluida solo en el contexto si el contexto está presente en el prompt del usuario. La respuesta no hace referencia a ninguna información externa.\n",
    "Integridad: La respuesta responde completamente a la pregunta con suficiente detalle.\n",
    "Fluidez: La respuesta está bien organizada y es fácil de leer.\n",
    "\n",
    "## Rúbrica de Calificación\n",
    "\"A\": La Respuesta A responde a la pregunta dada según los criterios mejor que la respuesta B.\n",
    "\"SAME\": La Respuesta A y B responden a la pregunta dada igualmente bien según los criterios.\n",
    "\"B\": La Respuesta B responde a la pregunta dada según los criterios mejor que la respuesta A.\n",
    "\n",
    "## Pasos de Evaluación\n",
    "PASO 1: Analiza la Respuesta A según los criterios de calidad de preguntas y respuestas: Determina qué tan bien la Respuesta A cumple con los requisitos del usuario, está fundamentada en el contexto, es completa y fluida, y proporciona una evaluación según el criterio.\n",
    "PASO 2: Analiza la Respuesta B según los criterios de calidad de preguntas y respuestas: Determina qué tan bien la Respuesta B cumple con los requisitos del usuario, está fundamentada en el contexto, es completa y fluida, y proporciona una evaluación según el criterio.\n",
    "PASO 3: Compara el rendimiento general de la Respuesta A y la Respuesta B según tus análisis y evaluación.\n",
    "PASO 4: Salida de tu preferencia de \"A\", \"SAME\" o \"B\" en el campo pairwise_choice según la Rúbrica de Calificación.\n",
    "PASO 5: Salida de tu razonamiento de evaluación en el campo explanation.\n",
    "\n",
    "# Entradas del Usuario y Respuestas Generadas por la IA\n",
    "## Entradas del Usuario\n",
    "### Instrucción\n",
    "{prompt}\n",
    "\n",
    "# Respuestas Generadas por la IA\n",
    "\n",
    "### Respuesta A\n",
    "{baseline_model_response}\n",
    "\n",
    "### Respuesta B\n",
    "{response}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class AnswerComparison(enum.Enum):\n",
    "  A = 'A'\n",
    "  SAME = 'SAME'\n",
    "  B = 'B'\n",
    "\n",
    "\n",
    "@functools.cache\n",
    "def eval_pairwise(prompt, response_a, response_b, n=1):\n",
    "  \"\"\"Determina cuál de dos respuestas a la misma pregunta es mejor.\"\"\"\n",
    "\n",
    "  chat = client.chats.create(model='gemini-2.0-flash')\n",
    "\n",
    "  # Genera la respuesta de texto completa.\n",
    "  response = chat.send_message(\n",
    "      message=QA_PAIRWISE_PROMPT.format(\n",
    "          prompt=[prompt, document_file],\n",
    "          baseline_model_response=response_a,\n",
    "          response=response_b)\n",
    "  )\n",
    "  verbose_eval = response.text\n",
    "\n",
    "  # Coerce into the desired structure.\n",
    "  structured_output_config = types.GenerateContentConfig(\n",
    "      response_mime_type=\"text/x.enum\",\n",
    "      response_schema=AnswerComparison,\n",
    "  )\n",
    "  response = chat.send_message(\n",
    "      message=\"Convert the final score.\",\n",
    "      config=structured_output_config,\n",
    "  )\n",
    "  structured_eval = response.parsed\n",
    "\n",
    "  return verbose_eval, structured_eval\n",
    "\n",
    "\n",
    "question = questions[0]\n",
    "answer_a = answer_question(question, terse_guidance)\n",
    "answer_b = answer_question(question, cited_guidance)\n",
    "\n",
    "text_eval, struct_eval = eval_pairwise(\n",
    "    prompt=question,\n",
    "    response_a=answer_a,\n",
    "    response_b=answer_b,\n",
    ")\n",
    "\n",
    "display(Markdown(text_eval))\n",
    "print(struct_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O74EgXn4oiqn"
   },
   "source": [
    "Con un evaluador por pares en su lugar, lo único que se requiere para clasificar los prompts entre sí es un comparador.\n",
    "\n",
    "Este ejemplo implementa los comparadores mínimos requeridos para el orden total (`==` y `<`) y realiza la comparación utilizando `n_iterations` evaluaciones sobre el conjunto de `questions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T09:32:40.226997Z",
     "iopub.status.busy": "2025-03-31T09:32:40.226614Z",
     "iopub.status.idle": "2025-03-31T09:32:40.236292Z",
     "shell.execute_reply": "2025-03-31T09:32:40.235190Z",
     "shell.execute_reply.started": "2025-03-31T09:32:40.226961Z"
    },
    "id": "Sr1PnWkZ33Cu"
   },
   "outputs": [],
   "source": [
    "@functools.total_ordering\n",
    "class QAGuidancePrompt:\n",
    "  \"\"\"Un prompt de orientación para preguntas y respuestas o instrucción del sistema.\"\"\"\n",
    "\n",
    "  def __init__(self, prompt, questions, n_comparisons=NUM_ITERATIONS):\n",
    "    \"\"\"Crea el prompt. Proporciona preguntas para evaluar y el número de evaluaciones a realizar.\"\"\"\n",
    "    self.prompt = prompt\n",
    "    self.questions = questions\n",
    "    self.n = n_comparisons\n",
    "\n",
    "  def __str__(self):\n",
    "    return self.prompt\n",
    "\n",
    "  def _compare_all(self, other):\n",
    "    \"\"\"Compara dos prompts en todas las preguntas durante n pruebas.\"\"\"\n",
    "    results = [self._compare_n(other, q) for q in questions]\n",
    "    mean = sum(results) / len(results)\n",
    "    return round(mean)\n",
    "\n",
    "  def _compare_n(self, other, question):\n",
    "    \"\"\"Compara dos prompts en una pregunta durante n pruebas.\"\"\"\n",
    "    results = [self._compare(other, question, n) for n in range(self.n)]\n",
    "    mean = sum(results) / len(results)\n",
    "    return mean\n",
    "\n",
    "  def _compare(self, other, question, n=1):\n",
    "    \"\"\"Compara dos prompts en una sola pregunta.\"\"\"\n",
    "    answer_a = answer_question(question, self.prompt)\n",
    "    answer_b = answer_question(question, other.prompt)\n",
    "\n",
    "    _, result = eval_pairwise(\n",
    "        prompt=question,\n",
    "        response_a=answer_a,\n",
    "        response_b=answer_b,\n",
    "        n=n,  # Cache buster\n",
    "    )\n",
    "    # print(f'q[{question}], a[{self.prompt[:20]}...], b[{other.prompt[:20]}...]: {result}')\n",
    "\n",
    "    # Convierte el enum a los valores de comparación numérica estándar de Python.\n",
    "    if result is AnswerComparison.A:\n",
    "      return 1\n",
    "    elif result is AnswerComparison.B:\n",
    "      return -1\n",
    "    else:\n",
    "      return 0\n",
    "\n",
    "  def __eq__(self, other):\n",
    "    \"\"\"Verificación de igualdad que realiza una evaluación por pares.\"\"\"\n",
    "    if not isinstance(other, QAGuidancePrompt):\n",
    "      return NotImplemented\n",
    "\n",
    "    return self._compare_all(other) == 0\n",
    "\n",
    "  def __lt__(self, other):\n",
    "    \"\"\"Verificación de orden que realiza una evaluación por pares.\"\"\"\n",
    "    if not isinstance(other, QAGuidancePrompt):\n",
    "      return NotImplemented\n",
    "\n",
    "    return self._compare_all(other) < 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-T7gpxy6cCI"
   },
   "source": [
    "Ahora las funciones de ordenamiento de Python \"simplemente funcionarán\" en cualquier instancia de `QAGuidancePrompt`. Las funciones `answer_question` y `eval_pairwise` están [memoizadas](https://es.wikipedia.org/wiki/Memoizaci%C3%B3n) para evitar generar innecesariamente las mismas respuestas o evaluaciones, por lo que deberías ver que esto se completa rápidamente a menos que hayas cambiado las preguntas, los prompts o el número de iteraciones de los pasos anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T09:32:40.237624Z",
     "iopub.status.busy": "2025-03-31T09:32:40.237348Z",
     "iopub.status.idle": "2025-03-31T09:32:51.191075Z",
     "shell.execute_reply": "2025-03-31T09:32:51.190082Z",
     "shell.execute_reply.started": "2025-03-31T09:32:40.237602Z"
    },
    "id": "DAT1wp--sOfS",
    "outputId": "29c832c3-b3c2-4bfe-ce45-e7d2cd841000"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1: Answer the following question in a single sentence, or as close to that as possible.\n",
      "---\n",
      "#2: Provide a thorough, detailed answer to the following question, citing the document and supplying additional background information as much as possible.\n",
      "---\n",
      "#3: Provide a brief answer to the following question, use a citation if necessary, but only enough to answer the question.\n"
     ]
    }
   ],
   "source": [
    "terse_prompt = QAGuidancePrompt(terse_guidance, questions)\n",
    "moderate_prompt = QAGuidancePrompt(moderate_guidance, questions)\n",
    "cited_prompt = QAGuidancePrompt(cited_guidance, questions)\n",
    "\n",
    "# Ordena en orden inverso, para que el mejor esté primero\n",
    "sorted_results = sorted([terse_prompt, moderate_prompt, cited_prompt], reverse=True)\n",
    "for i, p in enumerate(sorted_results):\n",
    "  if i:\n",
    "    print('---')\n",
    "\n",
    "  print(f'#{i+1}: {p}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfphETryZqdo"
   },
   "source": [
    "## Desafíos\n",
    "\n",
    "### Limitaciones de los LLMs\n",
    "\n",
    "Se sabe que los LLMs tienen problemas en ciertas tareas, y estos desafíos aún persisten al usar LLMs como evaluadores. Por ejemplo, los LLMs pueden tener dificultades para contar el número de caracteres en una palabra (esto es un problema numérico, no un problema de lenguaje), por lo que un evaluador LLM no podrá evaluar con precisión este tipo de tarea. Hay soluciones disponibles en algunos casos, como conectar herramientas para manejar problemas no adecuados para un modelo de lenguaje, pero es importante que entiendas las posibles limitaciones e incluyas evaluadores humanos para calibrar tu sistema de evaluación y determinar una línea base.\n",
    "\n",
    "Una razón por la que los evaluadores LLM funcionan bien es que toda la información que necesitan está disponible en el contexto de entrada, por lo que el modelo solo necesita atender a esa información para producir el resultado. Al personalizar prompts de evaluación o construir tus propios sistemas, ten esto en cuenta y asegúrate de no depender del \"conocimiento interno\" del modelo, o de un comportamiento que podría ser mejor proporcionado por una herramienta.\n",
    "\n",
    "### Mejorando la confianza\n",
    "\n",
    "Una forma de mejorar la confianza de tus evaluaciones es incluir un conjunto diverso de evaluadores. Es decir, usa los mismos prompts y salidas, pero ejecútalos en diferentes modelos, como Gemini Flash y Pro, o incluso entre diferentes proveedores, como Gemini, Claude, ChatGPT y modelos locales como Gemma o Qwen. Esto sigue la misma idea utilizada anteriormente, donde repetir pruebas para reunir múltiples \"opiniones\" ayuda a [reducir el error](https://es.wikipedia.org/wiki/Ley_de_los_grandes_n%C3%BAmeros), excepto que al usar diferentes modelos las \"opiniones\" serán más diversas.\n",
    "\n",
    "\n",
    "## Aprende más\n",
    "\n",
    "Para aprender más sobre los sistemas de evaluación, consulta [esta guía](https://cloud.google.com/blog/products/ai-machine-learning/enhancing-llm-quality-and-interpretability-with-the-vertex-gen-ai-evaluation-service?e=48754805) centrada en la evaluación utilizando el Servicio de Evaluación de Gen AI de Google Cloud.\n",
    "\n",
    "Y asegúrate de leer el **documento técnico adicional** sobre [Evaluación de Modelos de Lenguaje Grandes](https://services.google.com/fh/files/blogs/neurips_evaluation.pdf).\n",
    "\n",
    "*- [Mark McD](https://linktr.ee/markmcd)*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
